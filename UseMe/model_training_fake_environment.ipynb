{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Introduction\n\n\n\nThis notebook is a template for training a PPO model in fake environment. Fake environment is a mapping of Webots grid world. The model trained in fake environment can be applied in Webots grid world. \n\n---\n**NOTE**\n\nTo use this notebook, please first follow `UseGuide.md` to install the neccessary packages.\n\n---"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'stable_baselines'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-cdd7d65bdead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPPO1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"]}],"source":"%%capture output \n# captures ALL output in cell to disable tensorflow warnings\n\nimport numpy as np\nfrom stable_baselines import PPO1"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"import sys\nsys.path.insert(0,'../backend')\n\n# load our fakegym\nfrom fakegym.fakeenv import WbtGymFake\nfrom fakegym.state import FakeState"},{"cell_type":"markdown","execution_count":0,"metadata":{},"outputs":[],"source":"# Create a Custom Reward Function (optional)\n\nYou can create your own reward function and check done function in this block. These following variables and methods are for your useï¼š\n* ` `\n* ` `\n* ` `\n\n\nHere is a quick example of `calc_reward()` and `check_done()`:\n```python\n\n```"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"class MyEnv(WbtGymFake):\n    def __init__(self, seed=None, N=10, num_of_sensors=4, obstacles_num=16, step_range=(1, 1), obs=FakeState, obs_len=1):\n                  super(MyEnv, self).__init__(seed, N, num_of_sensors, obstacles_num, step_range, obs, obs_len=obs_len)\n\n    def calc_reward(self):\n            pass\n           \n    def check_done(self):\n        \"\"\"Check done\"\"\"\n            pass"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Select Parameters for the training environment\n\nYou can setup the environment parameters for your training:\n\n* `world_size` , setup the size of environments for training. For example: `world_size = 10` will setup a square map of size 10x10. \n* `num_obstacles`, setup the number of obstacles. Each obstacle is a block of size 1x1."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"world_size = 10\nnum_obstacles = 16"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Start our Webotsgym\n\n\n"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"# normal\nenv = WbtGymFake(N=world_size, obstacles_num=num_obstacles)\nenv.render()"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"# # custom reward class\n# env = MyEnv(N=world_size, obstacles_num=num_obstacles)\n# env.render()"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Initialize a Model from Stable Baselines\n\nMore information of setting parameters for PPO model can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"%%capture output \n# captures ALL output in cell to disable tensorflow warnings\n\nmodel_name = \"PPO_fakeenv\"\nmodel = PPO1(\"MlpPolicy\", env)"},{"cell_type":"markdown","metadata":{},"source":"# Train a Model on the fake environment\n\n\nTrain and a PPO model on the fake environment and save it after training. Please setup the training parameters:\n\n* `time_steps`, the total number of samples to train on.\n\nMore information of setting parameters for model training can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"time_steps = 100000\nmodel.learn(total_timesteps=time_steps)\nmodel.save(\"model/grid/{}\".format(model_name))\nprint(\"training finished\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}