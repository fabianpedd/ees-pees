{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "This notebook is a template to train a PPO model in our fake environment. The fake environment is a mapping of the Webots grid world where the model can be used.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "To use this notebook, please first follow `UseGuide.md` to install the neccessary packages.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output \n",
    "# captures ALL output in cell to disable tensorflow warnings\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines import PPO1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../backend')\n",
    "\n",
    "# load our fakegym\n",
    "from fakegym.fakeenv import WbtGymFake\n",
    "from fakegym.state import FakeState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Reward Function (optional)\n",
    "\n",
    "You can create your own reward function and check done function in this block. These following attributes and methods are for your useï¼š\n",
    "* `gps_actual`,  get gps data of the current position.\n",
    "* `gps_target`, get gps data of the target.\n",
    "* `get_target_distance(normalized=False)`, calculate euclidean distance from the current position to the target. `normalized` - (bool) If True, the distance will be normalized into the ratio of the actual distance to the maximum distance.\n",
    "* `gps_visited_count`, the number of times this position was reached in this episode.\n",
    "* `obs.touching`, whether the agent touches an obstacle or not.\n",
    "* `time_steps`, how many time steps the agent has used in this episode.\n",
    "\n",
    "The reward function and check done function in the cell below can be used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(WbtGymFake):\n",
    "    def __init__(self, seed=None, N=10, num_of_sensors=4, obstacles_num=16, step_range=(1, 1), obs=FakeState, obs_len=1):\n",
    "                  super(MyEnv, self).__init__(seed, N, num_of_sensors, obstacles_num, step_range, obs, obs_len=obs_len)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        if self.gps_actual == self.gps_target:\n",
    "            reward = 10000\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "            # step penalty\n",
    "            target_distance = self.get_target_distance(normalized=True)\n",
    "            step_penalty = -1\n",
    "            lambda_ = 5\n",
    "            reward += step_penalty * (1 - np.exp(-lambda_ * target_distance))\n",
    "\n",
    "            # visited count penalty\n",
    "            vc = self.gps_visited_count\n",
    "            if vc > 3:\n",
    "                reward += -0.2 * (vc - 2)**2\n",
    "\n",
    "            # touching penalty\n",
    "            if self.obs.touching is True:\n",
    "                reward -= 500\n",
    "\n",
    "        return reward\n",
    "           \n",
    "    def check_done(self):\n",
    "        if self.time_steps == 200:\n",
    "            return True\n",
    "        if self.gps_actual == self.gps_target:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Parameters for the Training Environment\n",
    "\n",
    "You can setup the environment parameters for your training:\n",
    "\n",
    "* `world_size` , setup the size of the training environment. For example: `world_size = 10` will setup a square map of size 10x10. \n",
    "* `num_obstacles`, setup the number of obstacles. Each obstacle is a block of size 1x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 10\n",
    "num_obstacles = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Fake Environment\n",
    "\n",
    "\n",
    "The training environment will be created. If you want to use the custom reward class created above, please use custom cell and comment the standard cell. The situation of the fake environment will be plotted. Yellow square represent the target, green square is the current position and the light blue parts are the obstacles. Around the map there is a wall with a thickness of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "env = WbtGymFake(N=world_size, obstacles_num=num_obstacles)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # custom\n",
    "# env = MyEnv(N=world_size, obstacles_num=num_obstacles)\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Model from Stable-Baselines\n",
    "\n",
    "More information of setting parameters for PPO model can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output \n",
    "# captures ALL output in cell to disable tensorflow warnings\n",
    "\n",
    "model_name = \"PPO_fakeenv\"\n",
    "model = PPO1(\"MlpPolicy\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model on the Fake Environment\n",
    "\n",
    "\n",
    "Train a PPO model on the fake environment and save it after training. Please setup the training parameters:\n",
    "\n",
    "Please setup the number of `time_steps` you want to train.\n",
    "\n",
    "More information of setting parameters for model training can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_steps = 100000\n",
    "model.learn(total_timesteps=time_steps)\n",
    "model.save('model/grid/{}'.format(model_name))\n",
    "print ('Training finished :)')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
