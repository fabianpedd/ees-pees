{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a template for training a PPO model in Webots continuous environment. In this notebook you are able to create your own reward function, setup environment, train and save model.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "To use this notebook, please first follow `UseGuide.md` to install the neccessary software and packages.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output \n",
    "# captures ALL output in cell to disable tensorflow warnings\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines import PPO1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../backend')\n",
    "\n",
    "# load our webotsgym\n",
    "import webotsgym as wg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Reward Function (optional)\n",
    "\n",
    "You can create your own reward function and check done function in this block. These following variables and methods are for your useï¼š\n",
    "* `env.gps_actual`, get gps data for the current position.\n",
    "* `env.gps_target`, get gps data for the target.\n",
    "* `env.get_target_distance(normalized=False)`, calculate euclidean distance from the current position to target. `normalized` - (bool) If True, the distance will be normalized into the ratio of the actual distance to the maximum distance.\n",
    "* `env.state.touching`, whether the agent touchs a obstacle or not.\n",
    "* `env.steps_in_run`, number of total timesteps of the current run..\n",
    "\n",
    "Here is a quick example of `calc_reward()` and `check_done()`:\n",
    "```python\n",
    "    def calc_reward(self):\n",
    "        if self.env.get_target_distance() < 0.05:\n",
    "            reward = 10000\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.get_target_distance() < 0.05:\n",
    "            return True\n",
    "        if self.env.steps_in_run % 2500 == 0:\n",
    "            return True\n",
    "        return False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEval(wg.WbtReward):\n",
    "    def __init__(self, env, config: wg.WbtConfig = wg.WbtConfig()):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        \"\"\"calculate the reward of the current state\n",
    "           Returns: (double) value of reward\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def check_done(self):\n",
    "        \"\"\"check whether this episode should be ended or not\n",
    "           Returns: (bool) end this episode or not\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Parameters for the Webots World\n",
    "\n",
    "You can setup the Webots environment parameters for your training:\n",
    "\n",
    "* `config.world_size` , setup the size of Webots environments for training. For example: `config.world_size = 8` will setup a square map of size 8x8 in Webots.\n",
    "* `config.num_obstacles`, setup the number of obstacles. Each obstacle is a block of size 1x1.\n",
    "* `config.sim_mode`, used to setup the speed of the simulation of Webots. \n",
    "`config.sim_mode = wg.config.SimSpeedMode.NORMAL`, run the simulation in the Real-Time mode.\n",
    "`config.sim_mode = wg.config.SimSpeedMode.RUN`, run the simulation as fast as possible using all the available CPU power. \n",
    "`config.sim_mode = wg.config.SimSpeedMode.FAST`, run the simulation as fast as possible without the graphics rendering, hence the 3d window is black.\n",
    "* `config.sim_step_every_x` is used to control the frequency of state/action interactions between the robot and Webots, measured in Webots time-stpes (32ms). The lower the sim_step_every_x, the more frequent the robot's actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wg.WbtConfig()\n",
    "config.world_size = 3\n",
    "config.num_obstacles = 0\n",
    "config.sim_mode = wg.config.SimSpeedMode.RUN\n",
    "config.sim_step_every_x = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start our Webotsgym\n",
    "\n",
    "\n",
    "The training environment will be created. If you want, you can use the custom reward class created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accepting on Port:  10201\n"
    }
   ],
   "source": [
    "# set eval class (choose custom or standard)\n",
    "# eval_ = MyEval  # custom\n",
    "eval_ = wg.WbtReward  # standard\n",
    "\n",
    "env = wg.WbtGym(config=config,\n",
    "                    evaluate_class=eval_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Model from Stable Baselines\n",
    "\n",
    "More information of setting parameters for PPO model can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output \n",
    "# captures ALL output in cell to disable tensorflow warnings\n",
    "\n",
    "model_name = \"PPO_webots_continuous\"\n",
    "model = PPO1(\"MlpPolicy\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model on the Webotsgym\n",
    "\n",
    "\n",
    "Train and a PPO model on the Webots continuous environment and save it after training. Please setup the training parameters:\n",
    "\n",
    "* `time_steps`, the total timesteps of samples to train on.\n",
    "\n",
    "More information of setting parameters for model training can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training finished :)\n"
    }
   ],
   "source": [
    "time_steps = 100000\n",
    "model.learn(total_timesteps=time_steps)\n",
    "model.save(\"model/continuous/{}\".format(model_name))\n",
    "print(\"Training finished :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}