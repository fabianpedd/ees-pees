{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","This notebook is a template for training a PPO model in Webots grid environment. In this notebook you are able to create your own reward function, setup environment, train and save model.\n","\n","---\n","**NOTE**\n","\n","To use this notebook, please first follow `UseGuide.md` to install the neccessary software and packages.\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%%capture output \n","# captures ALL output in cell to disable tensorflow warnings\n","\n","import numpy as np\n","from stable_baselines import PPO1\n","from stable_baselines.common.callbacks import CheckpointCallback"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.insert(0,'../backend')\n","\n","# load our webotsgym\n","import webotsgym as wg"]},{"cell_type":"markdown","metadata":{},"source":["# Create a Custom Reward Function (optional)\n","\n","You can create your own reward function and check done function in this block. These following variables and methods are for your useï¼š\n","* `env.gps_actual`, get gps data for the current position.\n","* `env.gps_target`, get gps data for the target.\n","* `env.get_target_distance(normalized=False)`, calculate euclidean distance from the current position to target. `normalized` - (bool) If True, the distance will be normalized into the ratio of the actual distance to the maximum distance.\n","* `env.gps_visited_count`, the frequency of reaching the current position in the past steps\n","* `env.state.touching`, whether the agent touchs a obstacle or not.\n","* `env.steps_in_run`, how many time steps the agent has used in this episode.\n","* `env.total_reward`, the sum of the reward in this episode\n","* `targetband`, the threshold for judging whether the robot reachs the target or not. The default value is 0.05.\n","\n","The  reward function and check done function in the cell below is an example for you."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class MyEval(wg.WbtRewardGrid):\n","    def __init__(self, env, config, targetband=0.05):\n","        super(MyEval, self).__init__(env, config)\n","        self.targetband = targetband\n","\n","    def calc_reward(self):\n","        if self.env.get_target_distance() < self.targetband:\n","            reward = 10000\n","        else:\n","            reward = 0\n","\n","            # step penalty\n","            target_distance = self.env.get_target_distance(normalized=True)\n","            step_penalty = -1\n","            lambda_ = 5\n","            reward += step_penalty * (1 - np.exp(-lambda_ * target_distance))\n","\n","            # visited count penalty\n","            vc = self.env.gps_visited_count\n","            if vc > 3:\n","                reward += -0.2 * (vc - 2)**2\n","\n","            # touching penalty\n","            if self.env.state.touching is True:\n","                reward -= 500\n","\n","        return reward\n","\n","    def check_done(self):\n","        if self.env.steps_in_run == 200:\n","            return True\n","        if self.env.total_reward < -1000:\n","            return True\n","        if self.env.get_target_distance() < self.targetband:\n","            return True\n","        return False"]},{"cell_type":"markdown","metadata":{},"source":["# Select Parameters for the Webots World\n","\n","You can setup the Webots environment parameters for your training:\n","\n","* `config.world_size` , setup the size of Webots environments for training. For example: `config.world_size = 8` will setup a square map of size 8x8 in Webots.\n","* `config.num_obstacles`, setup the number of obstacles. Each obstacle is a block of size 1x1.\n","* `config.sim_mode`, used to setup the speed of the simulation of Webots. \n","`config.sim_mode = wg.config.SimSpeedMode.NORMAL`, run the simulation in the Real-Time mode.\n","`config.sim_mode = wg.config.SimSpeedMode.RUN`, run the simulation as fast as possible using all the available CPU power. \n","`config.sim_mode = wg.config.SimSpeedMode.FAST`, run the simulation as fast as possible without the graphics rendering, hence the 3d window is black.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["config = wg.WbtConfig()\n","config.world_size = 8\n","config.num_obstacles = 16\n","config.sim_mode = wg.config.SimSpeedMode.FAST"]},{"cell_type":"markdown","metadata":{},"source":["# Start our Webotsgym\n","\n","\n","The training environment will be created. If you want, you can use the custom reward class created above."]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Accepting on Port:  10201\n"}],"source":["# set eval class  (choose custom or standard)\n","# eval_ = MyEval  # custom\n","eval_ = wg.WbtRewardGrid  # standard\n","\n","env = wg.WbtGymGrid(config=config,\n","                    evaluate_class=eval_)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize a Model from Stable-Baselines\n","\n","During training the model will be saved in `UseMe/model/grid/.log/` periodically. If you want to start your training from the trained model, please comment `new model` and use `from a trained model` .\n","\n","More information of setting parameters for PPO model can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["%%capture output \n","# captures ALL output in cell to disable tensorflow warnings\n","\n","model_name = \"PPO_webots_grid\"\n","checkpoint_callback = CheckpointCallback(save_freq=5000, save_path='model/grid/.log/',\n","                                         name_prefix=model_name)\n","\n","# new model\n","model = PPO1(\"MlpPolicy\", env) \n","\n","# # from a trained model\n","# load_model = \"name_of _model\"\n","# model = PPO1.load(\"model/grid/.log/{}\".format(load_model), env)"]},{"cell_type":"markdown","metadata":{},"source":["# Train a Model on the Webotsgym\n","\n","\n","Train and a PPO model on the Webots grid environment and save it after training. Please setup the training parameters:\n","\n","* `time_steps`, the total number of samples to train on.\n","\n","More information of setting parameters for model training can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)."]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Training finished :)\n"}],"source":["time_steps = 100000\n","model.learn(total_timesteps=time_steps, callback=checkpoint_callback)\n","model.save('model/grid/{}'.format(model_name))\n","print ('Training finished :)')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python_defaultSpec_1595074799946","display_name":"Python 3.6.10 64-bit ('spinningup': conda)"}},"nbformat":4,"nbformat_minor":2}