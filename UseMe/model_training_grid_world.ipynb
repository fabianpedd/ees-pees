{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","This notebook is a template for training a PPO model in Webots grid environment. In this notebook you are able to create your own reward function, setup environment, train and save model.\n","\n","---\n","**NOTE**\n","\n","To use this notebook, please first follow `UseGuide.md` to install the neccessary software and packages.\n","\n","---"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["%%capture output \n","# captures ALL output in cell to disable tensorflow warnings\n","\n","import numpy as np\n","from stable_baselines import PPO1"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.insert(0,'../backend')\n","\n","# load our webotsgym\n","import webotsgym as wg"]},{"cell_type":"markdown","metadata":{},"source":["# Create a Custom Reward Function (optional)\n","\n","You can create your own reward function and check done function in this block. These following variables and methods are for your useï¼š\n","* `env.gps_actual`, get gps data for the current position.\n","* `env.gps_target`, get gps data for the target.\n","* `env.get_target_distance(normalized=False)`, calculate euclidean distance from the current position to target. `normalized` - (bool) If True, the distance will be normalized into the ratio of the actual distance to the maximum distance.\n","* `env.gps_visited_count`, the frequency of reaching the current position in the past steps\n","* `env.state.touching`, whether the agent touchs a obstacle or not.\n","* `env.steps_in_run`, how many time steps the agent has used in this episode.\n","* `env.total_reward`, the sum of the reward in this episode\n","* `targetband`, the threshold for judging whether the robot reachs the target or not. The default value is 0.05.\n","\n","The  reward function and check done function in the cell below is an example for you."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class MyEval(wg.WbtRewardGrid):\n","    def __init__(self, env, config, targetband=0.05):\n","        super(MyEval, self).__init__(env, config)\n","        self.targetband = targetband\n","\n","    def calc_reward(self):\n","        if self.env.get_target_distance() < self.targetband:\n","            reward = 10000\n","        else:\n","            reward = 0\n","\n","            # step penalty\n","            target_distance = self.env.get_target_distance(normalized=True)\n","            step_penalty = -1\n","            lambda_ = 5\n","            reward += step_penalty * (1 - np.exp(-lambda_ * target_distance))\n","\n","            # visited count penalty\n","            vc = self.env.gps_visited_count\n","            if vc > 3:\n","                reward += -0.2 * (vc - 2)**2\n","\n","            # touching penalty\n","            if self.env.state.touching is True:\n","                reward -= 500\n","\n","        return reward\n","\n","    def check_done(self):\n","        if self.env.steps_in_run == 200:\n","            return True\n","        if self.env.total_reward < -1000:\n","            return True\n","        if self.env.get_target_distance() < self.targetband:\n","            return True\n","        return False"]},{"cell_type":"markdown","metadata":{},"source":["# Select Parameters for the Webots World\n","\n","You can setup the Webots environment parameters for your training:\n","\n","* `config.world_size` , setup the size of Webots environments for training. For example: `config.world_size = 8` will setup a square map of size 8x8 in Webots.\n","* `config.num_obstacles`, setup the number of obstacles. Each obstacle is a block of size 1x1.\n","* `config.sim_mode`, used to setup the speed of the simulation of Webots. \n","`config.sim_mode = wg.config.SimSpeedMode.NORMAL`, run the simulation in the Real-Time mode.\n","`config.sim_mode = wg.config.SimSpeedMode.RUN`, run the simulation as fast as possible using all the available CPU power. \n","`config.sim_mode = wg.config.SimSpeedMode.FAST`, run the simulation as fast as possible without the graphics rendering, hence the 3d window is black.\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["config = wg.WbtConfig()\n","config.world_size = 8\n","config.num_obstacles = 16\n","config.sim_mode = wg.config.SimSpeedMode.RUN"]},{"cell_type":"markdown","metadata":{},"source":["# Start our Webotsgym\n","\n","\n","The training environment will be created. If you want, you can use the custom reward class created above."]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Accepting on Port:  10201\n"}],"source":["# set eval class  (choose custom or standard)\n","# eval_ = MyEval  # custom\n","eval_ = wg.WbtRewardGrid  # standard\n","\n","env = wg.WbtGymGrid(config=config,\n","                    evaluate_class=eval_)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize a Model from Stable-Baselines\n","\n","More information of setting parameters for PPO model can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["%%capture output \n","# captures ALL output in cell to disable tensorflow warnings\n","\n","model_name = \"PPO_webots_grid\"\n","model = PPO1(\"MlpPolicy\", env)"]},{"cell_type":"markdown","metadata":{},"source":["# Train a Model on the Webotsgym\n","\n","\n","Train and a PPO model on the Webots grid environment and save it after training. Please setup the training parameters:\n","\n","* `time_steps`, the total number of samples to train on.\n","\n","More information of setting parameters for model training can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"error","ename":"error","evalue":"unpack requires a buffer of 4 bytes","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-ed288cec8d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/grid/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Training finished :)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    240\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"********** Iteration %i ************\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miters_so_far\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;31m# Stop training early (triggered by the callback)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mtraj_segment_generator\u001b[0;34m(policy, env, horizon, reward_giver, gail, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mcurrent_ep_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/github/ees-pees/backend/webotsgym/env/grid/gridenv.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mAdditionally\u001b[0m \u001b[0mreinitialize\u001b[0m \u001b[0mvisited_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mlen_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_scaling\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/github/ees-pees/backend/webotsgym/env/webotenv.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupervisor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/github/ees-pees/backend/webotsgym/com/automate/supervisor.py\u001b[0m in \u001b[0;36mreset_environment\u001b[0;34m(self, seed, waiting_time)\u001b[0m\n\u001b[1;32m    150\u001b[0m                            0.0)\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_env_reset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/github/ees-pees/backend/webotsgym/com/automate/supervisor.py\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;34m\"\"\"Get current environment metadata.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPACKET_SIZE_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgps_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: unpack requires a buffer of 4 bytes"]}],"source":["time_steps = 100000\n","model.learn(total_timesteps=time_steps)\n","model.save('model/grid/{}'.format(model_name))\n","print ('Training finished :)')"]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python_defaultSpec_1595028080827","display_name":"Python 3.6.10 64-bit ('spinningup': conda)"}},"nbformat":4,"nbformat_minor":2}