{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Introduction\n\nThis notebook is a template for training a PPO model in Webots grid environment. In this notebook you are able to create your own reward function, setup environment, train and save model.\n\n---\n**NOTE**\n\nTo use this notebook, please first follow `UseGuide.md` to install the neccessary software and packages.\n\n---"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"%%capture output \n# captures ALL output in cell to disable tensorflow warnings\n\nimport numpy as np\nfrom stable_baselines import PPO1"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"import sys\nsys.path.insert(0,'../backend')\n\n# load our webotsgym\nimport webotsgym as wg"},{"cell_type":"markdown","metadata":{},"source":"# Create a Custom Reward Function (optional)\n\nYou can create your own reward function and check done function in this block. These following variables and methods are for your useï¼š\n* `env.gps_actual`, get gps data for the current position.\n* `env.gps_target`, get gps data for the target.\n* `env.get_target_distance(normalized=False)`, calculate euclidean distance from the current position to target. `normalized` - (bool) If True, the distance will be normalized into the ratio of the actual distance to the maximum distance.\n* `env.state.touching`, whether the agent touchs a obstacle or not.\n* `env.time_steps`, how many time steps the agent has used in this episode.\n\n\n\nHere is a quick example of `calc_reward()` and `check_done()`:\n```python\n    def calc_reward(self):\n        if self.env.get_target_distance() < 0.05:\n            reward = 10000\n        else:\n            reward = 0\n\n            # step penalty\n            target_distance = self.env.get_target_distance(normalized=True)\n            reward += -1 * (1 - exponential_decay(target_distance, lambda_=5))\n\n            # touching penalty\n            if self.env.state.touching is True:\n                reward -= 500\n\n        return reward\n\n    def check_done(self):\n        if self.env.time_steps == 200:\n            return True\n        if self.env.get_target_distance() < 0.05:\n            return True\n        return False\n```"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"class MyEval(wg.WbtRewardGrid):\n    def __init__(self, env, config: wg.WbtConfig = wg.WbtConfig()):\n        super(MyEval, self).__init__(env, config)\n\n    def calc_reward(self):\n        \"\"\"calculate the reward of the current state\n           Returns: (double) value of reward\n        \"\"\"\n        pass\n    \n    def check_done(self):\n        \"\"\"check whether this episode should be ended or not\n           Returns: (bool) end this episode or not\n        \"\"\"\n        pass"},{"cell_type":"markdown","metadata":{},"source":"# Select Parameters for the Webots World\n\nYou can setup the Webots environment parameters for your training:\n\n* `config.world_size` , setup the size of Webots environments for training. For example: `config.world_size = 8` will setup a square map of size 8x8 in Webots.\n* `config.num_obstacles`, setup the number of obstacles. Each obstacle is a block of size 1x1.\n* `config.sim_mode`, used to setup the speed of the simulation of Webots. \n`config.sim_mode = wg.config.SimSpeedMode.NORMAL`, run the simulation in the Real-Time mode.\n`config.sim_mode = wg.config.SimSpeedMode.RUN`, run the simulation as fast as possible using all the available CPU power. \n`config.sim_mode = wg.config.SimSpeedMode.FAST`, run the simulation as fast as possible without the graphics rendering, hence the 3d window is black.\n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"config = wg.WbtConfig()\nconfig.world_size = 8\nconfig.num_obstacles = 16\nconfig.sim_mode = wg.config.SimSpeedMode.RUN"},{"cell_type":"markdown","metadata":{},"source":"# Start our Webotsgym\n\n\nThe training environment will be created. If you want to use your own reward class, please use `env = wg.WbtGymGrid(config=config,evaluate_class=MyEval)` and comment `env = wg.WbtGymGrid(config=config)`."},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accepting on Port:  10201\n"]}],"source":"# normal\nenv = wg.WbtGymGrid(config=config)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"# custom reward class\n# env = wg.WbtGymGrid(config=config,\n#                     evaluate_class=MyEval)"},{"cell_type":"markdown","metadata":{},"source":"# Initialize a Model from Stable Baselines\n\nMore information of setting parameters for PPO model can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"%%capture output \n# captures ALL output in cell to disable tensorflow warnings\n\nmodel_name = \"PPO_webots_grid\"\nmodel = PPO1(\"MlpPolicy\", env)"},{"cell_type":"markdown","metadata":{},"source":"# Train a Model on the Webotsgym\n\n\nTrain and a PPO model on the Webots grid environment and save it after training. Please setup the training parameters:\n\n* `time_steps`, the total number of samples to train on.\n\nMore information of setting parameters for model training can be find [here](https://stable-baselines.readthedocs.io/en/master/modules/ppo1.html#parameters)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n","3\n","2\n","1\n","1\n","3\n","1\n","3\n","2\n","3\n"]}],"source":"time_steps = 100000\nmodel.learn(total_timesteps=time_steps)\nmodel.save(\"model/grid/{}\".format(model_name))"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}