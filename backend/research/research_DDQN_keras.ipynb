{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import environment\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(environment)\n",
    "e = environment.DQNEnv()\n",
    "e.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Management\n",
    "NN1 = False\n",
    "NN2 = True\n",
    "\n",
    "#define radius\n",
    "radius = 3\n",
    "mx = (radius*2+1)**2 + 4  # number fields + coordinates position & finish\n",
    "my = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN (don't work so far)\n",
    "if(NN1): \n",
    "    model = keras.Sequential()\n",
    "    # input ...\n",
    "    model.add(keras.Input(shape=(29,)))\n",
    "    model.add(layers.Dense(20, activation=\"relu\"))\n",
    "    model.add(layers.Dense(20, activation=\"relu\"))\n",
    "    #model.add(layers.Dense(20, activation=\"softmax\"))\n",
    "    model.add(layers.Dense(4))\n",
    "\n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "    sgd = tf.keras.optimizers.SGD(\n",
    "        learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\"\n",
    "    )\n",
    "    #model.compile(optimizer=sgd, loss = 'mse')\n",
    "    model.compile(SGD(lr=0.01), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(NN2):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.optimizers import SGD\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(mx*my,), activation='relu'))  # TODO: Use input as 2d array.\n",
    "    model.add(Dense(20, activation='relu'))  # TODO: Use input as 2d array.\n",
    "    model.add(Dense(4))\n",
    "    model.compile(SGD(lr=0.001), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon = 0.05  # probability to choose random action instead of best action\n",
    "gamma = 0.95\n",
    "batch_size = 150\n",
    "memory_size = 1000\n",
    "max_num_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []\n",
    "memory_lens = []\n",
    "history = []\n",
    "loss = []\n",
    "t_rewards = []\n",
    "t_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n",
      "Epoch 0 - loss 5.679988384246826 - aborted or dead after 100 steps - final reward -10 - total reward -640\n",
      "Epoch 1 - loss 2.224864959716797 - aborted or dead after 100 steps - final reward -10 - total reward -370\n",
      "Epoch 2 - loss 2.4586124420166016 - aborted or dead after 100 steps - final reward -10 - total reward -430\n",
      "Epoch 3 - loss 2.0327696800231934 - aborted or dead after 100 steps - final reward -10 - total reward -410\n",
      "Epoch 4 - loss 1.7409958839416504 - aborted or dead after 100 steps - final reward 0 - total reward -230\n",
      "Epoch 5 - loss 1.608498215675354 - aborted or dead after 100 steps - final reward -10 - total reward -390\n",
      "Epoch 6 - loss 1.324967622756958 - aborted or dead after 100 steps - final reward 0 - total reward -180\n",
      "Epoch 7 - loss 1.1661980152130127 - aborted or dead after 100 steps - final reward 0 - total reward -130\n",
      "Epoch 8 - loss 1.094338297843933 - aborted or dead after 100 steps - final reward 0 - total reward -10\n",
      "Epoch 9 - loss 0.9347092509269714 - aborted or dead after 100 steps - final reward 0 - total reward -10\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "len_ = 1\n",
    "num_epochs = 10\n",
    "startpos = e.get_pos()\n",
    "print(startpos)\n",
    "for epoch,epsilon in zip(range(num_epochs), np.linspace(1.,0.05, num_epochs)): \n",
    "    e.reset_pos(startpos)\n",
    "    state = e.get_state(radius)\n",
    "    done = False\n",
    "    dead = False\n",
    "    loss_epoch = []\n",
    "    rewards_epoch = []\n",
    "    time = 0\n",
    "    reward = 0\n",
    "    \n",
    "    while not done and not dead and time < max_num_steps: \n",
    "        time +=1\n",
    "        \n",
    "        if np.random.rand() <= epsilon: \n",
    "            action = np.random.randint(4)\n",
    "            if verbose: print('choosing random action {} ,'.format(action))\n",
    "        else: \n",
    "            Q = model.predict(state.reshape(1, -1))\n",
    "            action = np.argmax(Q)\n",
    "            if verbose: print('choosing best action {} ,'.format(action))\n",
    "                \n",
    "        n_state, reward, done, dead = e.step_f(action, radius)\n",
    "        if verbose: print('reward {}'.format(reward))\n",
    "        \n",
    "        memory.append([state,action,reward,n_state, done])\n",
    "        if verbose: print('Memory {}',format(memory[-1]))\n",
    "        \n",
    "        while len(memory) > memory_size:  # only keep the most recent experiences in memory\n",
    "            del memory[0]\n",
    "        \n",
    "        # Replay memories and train network.\n",
    "        actual_batch_size = min(batch_size, len(memory))\n",
    "        inputs = np.zeros((actual_batch_size, mx * my))\n",
    "        targets = np.zeros((actual_batch_size, 4))\n",
    "        \n",
    "        for i, j in enumerate(np.random.choice(len(memory), size=actual_batch_size, replace=False)):\n",
    "            s, a, r, new_s, d = memory[j]\n",
    "            inputs[i] = s.flatten()\n",
    "            if verbose: print(\"Inputs: \"+ str(inputs[i]))\n",
    "            Q = model.predict(s.reshape(1, -1))[0]\n",
    "            targets[i] = Q\n",
    "            if done:\n",
    "                targets[i, a] = r\n",
    "                if verbose: print(targets[i, a])\n",
    "            else:\n",
    "                Q_new = model.predict(new_s.reshape(1, -1))[0]\n",
    "                targets[i, a] = r + gamma * np.max(Q_new)\n",
    "                if verbose: print(\"Target:\" + str(targets[i, a]))\n",
    "        loss_epoch.append(model.train_on_batch(inputs, targets))\n",
    "        rewards_epoch.append(reward)\n",
    "        #print(np.sum(rewards_epoch))\n",
    "        state = n_state\n",
    "        \n",
    "    print('Epoch {} - loss {} - {} {} steps - final reward {} - total reward {}'.format(epoch, np.mean(loss_epoch), 'solved in' if done else 'aborted or dead after', time, reward, np.sum(rewards_epoch)))\n",
    "    loss.append(np.mean(loss_epoch))\n",
    "    t_rewards.append(np.sum(rewards_epoch))\n",
    "    t_time.append(time)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
