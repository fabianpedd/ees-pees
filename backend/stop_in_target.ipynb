{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import webotsgym as wg\n",
    "\n",
    "from webotsgym.config import WebotConfig\n",
    "from webotsgym.environment import WebotsEnv\n",
    "from webotsgym.evaluate import Evaluate, EvaluateMats, EvaluatePJ0\n",
    "from webotsgym.action import DiscreteAction, ContinuousAction\n",
    "from webotsgym.observation import Observation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n",
      "sending: env\n",
      "USE FAST MODE\n"
     ]
    }
   ],
   "source": [
    "def exponential_decay(x, N0=1, lambda_=5):\n",
    "    return N0*np.exp(-lambda_*x)\n",
    "\n",
    "class MyObs(Observation):\n",
    "    def __init__(self, env):\n",
    "        super(MyObs, self).__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "class MyEval(Evaluate):\n",
    "    def __init__(self, env, config: WebotConfig = WebotConfig()):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "        self.reward_range = (-1000, 1000)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        reward = -1\n",
    "        distance_norm = self.env.get_target_distance()\n",
    "        distance_abs = self.env.get_target_distance(False)\n",
    "\n",
    "        if distance_abs < 0.1 and abs(self.env.state.speed) < 0.05 and self.env.state.touching is False:\n",
    "            return 10000\n",
    "        else:\n",
    "            reward += 2500 * exponential_decay(distance_abs, lambda_=40) * exponential_decay(abs(self.env.state.speed), lambda_=10)\n",
    "        \n",
    "        if self.env.state.touching:\n",
    "            reward -= 5\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -10000:\n",
    "            print(\"reward boundary\")\n",
    "            return True\n",
    "        if self.env.get_target_distance(False) < 0.1 and abs(self.env.state.speed) < 0.05 and self.env.state.touching is False:\n",
    "            print(\"target reached\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "config = WebotConfig()\n",
    "config.fast_simulation = True\n",
    "config.reset_env_after = 200000\n",
    "config.num_obstacles = 0\n",
    "config.world_size = 2\n",
    "config.world_scaling = 0.5\n",
    "\n",
    "action_class = ContinuousAction(direction_type=\"steering\", relative=False)\n",
    "env = WebotsEnv(train=True, \n",
    "                action_class=action_class, \n",
    "                evaluate_class=MyEval,\n",
    "                observation_class=MyObs,\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/fabian/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fabian/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "time_steps = 499999\n",
    "model_name = \"Webots_find_target_small\"\n",
    "\n",
    "model = PPO1(\"MlpPolicy\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 250 )\t -0.9999498087560058\n",
      "Reward ( 500 )\t -0.9999966987649441\n",
      "Reward ( 750 )\t -5.999999983761102\n",
      "Reward ( 1000 )\t -5.999999992247886\n",
      "Reward ( 1250 )\t -5.999999977475733\n",
      "Reward ( 1500 )\t -0.9999987438624036\n",
      "Reward ( 1750 )\t -5.999999965798946\n",
      "Reward ( 2000 )\t -5.999999996824686\n",
      "Reward ( 2250 )\t -5.999999997876353\n",
      "Reward ( 2500 )\t -5.999999998165003\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 2750 )\t -0.9999896346677061\n",
      "Reward ( 3000 )\t -0.9999909493328931\n",
      "Reward ( 3250 )\t -5.999999939795154\n",
      "Reward ( 3500 )\t -5.999999994291154\n",
      "Reward ( 3750 )\t -0.9999559225493873\n",
      "Reward ( 4000 )\t -5.961438101721102\n",
      "Reward ( 4250 )\t -5.866573715230784\n",
      "Reward ( 4500 )\t -0.9899875771747109\n",
      "Reward ( 4750 )\t 0.08484045402007778\n",
      "Reward ( 5000 )\t -4.706538243769086\n",
      "Reward ( 5250 )\t 6.869443255375924\n",
      "Reward ( 5500 )\t -4.003439276071207\n",
      "Reward ( 5750 )\t -5.116717364220389\n",
      "Reward ( 6000 )\t -4.924271660116464\n",
      "Reward ( 6250 )\t -5.529368830724751\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 6500 )\t -0.9999990268079764\n",
      "Reward ( 6750 )\t -5.999999990478484\n",
      "Reward ( 7000 )\t -5.9999999931703405\n",
      "Reward ( 7250 )\t -5.999999994297385\n",
      "Reward ( 7500 )\t -5.999999997730033\n",
      "Reward ( 7750 )\t -5.999999995805289\n",
      "Reward ( 8000 )\t -5.999999998066197\n",
      "Reward ( 8250 )\t -5.999999998350196\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 8500 )\t -0.9999999999334422\n",
      "Reward ( 8750 )\t -5.9999999999978595\n",
      "Reward ( 9000 )\t -5.9999999999972085\n",
      "Reward ( 9250 )\t -5.999999999997628\n",
      "Reward ( 9500 )\t -5.9999999999996385\n",
      "Reward ( 9750 )\t -5.999999999999589\n",
      "Reward ( 10000 )\t -5.999999999999372\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 10250 )\t -0.9999962480471813\n",
      "Reward ( 10500 )\t -5.999991573570602\n",
      "Reward ( 10750 )\t -5.999990914299627\n",
      "Reward ( 11000 )\t -5.999994980563025\n",
      "Reward ( 11250 )\t -5.999991941004861\n",
      "Reward ( 11500 )\t -5.999994448900124\n",
      "Reward ( 11750 )\t -5.999994664668283\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 12000 )\t -0.9999999995205163\n",
      "Reward ( 12250 )\t -0.9999999877867467\n",
      "Reward ( 12500 )\t -5.999999999977686\n",
      "Reward ( 12750 )\t -5.999999999984988\n",
      "Reward ( 13000 )\t -5.999999999987814\n",
      "Reward ( 13250 )\t -5.999999999991897\n",
      "Reward ( 13500 )\t -5.99999999999443\n",
      "Reward ( 13750 )\t -5.999999999996889\n",
      "Reward ( 14000 )\t -5.9999999999957225\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 14250 )\t -0.9999999982792571\n",
      "Reward ( 14500 )\t -5.999999999795705\n",
      "Reward ( 14750 )\t -5.999999999456767\n",
      "Reward ( 15000 )\t -0.9999999467922783\n",
      "Reward ( 15250 )\t -5.999999999753961\n",
      "Reward ( 15500 )\t -5.9999999995905124\n",
      "Reward ( 15750 )\t -5.999999997944149\n",
      "Reward ( 16000 )\t -5.999999999463154\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 16250 )\t -0.9999999987797227\n",
      "Reward ( 16500 )\t -5.999999997567855\n",
      "Reward ( 16750 )\t -0.9999999881633991\n",
      "Reward ( 17000 )\t -5.999999999622754\n",
      "Reward ( 17250 )\t -0.9999997999299385\n",
      "Reward ( 17500 )\t -0.9993940406567436\n",
      "Reward ( 17750 )\t -0.9775775238557605\n",
      "target reached\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 18000 )\t -0.9999958389384402\n",
      "Reward ( 18250 )\t -0.9999345404936304\n",
      "Reward ( 18500 )\t -0.9999999278865059\n",
      "Reward ( 18750 )\t -0.9999994220233628\n",
      "Reward ( 19000 )\t -0.9999997105640753\n",
      "Reward ( 19250 )\t -5.9999999999618145\n",
      "Reward ( 19500 )\t -0.9999999921784476\n",
      "Reward ( 19750 )\t -5.999999999446263\n",
      "Reward ( 20000 )\t -5.999999999999335\n",
      "Reward ( 20250 )\t -5.9999999999102664\n",
      "Reward ( 20500 )\t -0.9999947792245374\n",
      "Reward ( 20750 )\t -0.9743114039677004\n",
      "Reward ( 21000 )\t -0.9999056252955479\n",
      "Reward ( 21250 )\t -0.9999537389519826\n",
      "Reward ( 21500 )\t -0.9532268695777899\n",
      "Reward ( 21750 )\t -0.9999999059143936\n",
      "Reward ( 22000 )\t -5.999999982348774\n",
      "reward boundary\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "sending: env\n",
      "USE FAST MODE\n",
      "Reward ( 22250 )\t -0.9999999997354794\n",
      "Reward ( 22500 )\t -0.9999999557266761\n",
      "Reward ( 22750 )\t -0.880476191618519\n",
      "Reward ( 23000 )\t -0.9999012621656286\n",
      "Reward ( 23250 )\t -0.9998722351597624\n",
      "Reward ( 23500 )\t -0.9990597627075848\n",
      "Reward ( 23750 )\t 7.30888785406936\n",
      "Reward ( 24000 )\t -0.9893572742807121\n",
      "Reward ( 24250 )\t -0.9999999087491677\n",
      "Reward ( 24500 )\t -5.999999999933983\n",
      "Reward ( 24750 )\t -0.999999998575041\n",
      "Reward ( 25000 )\t -0.9999943784758901\n",
      "Reward ( 25250 )\t -5.999999999999278\n",
      "Reward ( 25500 )\t -0.9999993443189803\n",
      "Reward ( 25750 )\t -5.999999994936243\n",
      "Reward ( 26000 )\t -5.99999999995323\n",
      "Reward ( 26250 )\t -5.9999999999994\n",
      "reward boundary\n",
      "sending: env\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    model.learn(total_timesteps=50000, log_interval=10000)\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# model = PPO1.load(\"models/{}\".format('DQN_WebotFakeMini_TRPO_pj1_nReward2_200000'))\n",
    "# env = MyEnv()\n",
    "# obs = env.reset()\n",
    "\n",
    "# env.render()\n",
    "# done = False\n",
    "# max_num_steps = 100\n",
    "# time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO1.load(\"models/Webots_find_target_small\")\n",
    "\n",
    "def exponential_decay(x, N0=1, lambda_=5):\n",
    "    return N0*np.exp(-lambda_*x)\n",
    "\n",
    "class MyObs(Observation):\n",
    "    def __init__(self, env):\n",
    "        super(MyObs, self).__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "class MyEval(Evaluate):\n",
    "    def __init__(self, env, config: WebotConfig = WebotConfig()):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "        self.reward_range = (-1000, 1000)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        reward = -1\n",
    "        distance_norm = self.env.get_target_distance()\n",
    "        distance_abs = self.env.get_target_distance(False)\n",
    "\n",
    "        if distance_abs < 0.1 and abs(self.env.state.speed) < 0.05 and self.env.state.touching is False:\n",
    "            return 10000\n",
    "        else:\n",
    "            reward += 2500 * exponential_decay(distance_abs, lambda_=40) * exponential_decay(abs(self.env.state.speed), lambda_=10)\n",
    "        \n",
    "        if self.env.state.touching:\n",
    "             reward -= 5\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -10000:\n",
    "            print(\"reward boundary\")\n",
    "            return True\n",
    "        if self.env.get_target_distance(False) < 0.1 and abs(self.env.state.speed) < 0.05 and self.env.state.touching is False:\n",
    "            print(\"target reached\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "config = WebotConfig()\n",
    "config.fast_simulation = False\n",
    "config.reset_env_after = 200000\n",
    "config.num_obstacles = 0\n",
    "config.world_size = 2\n",
    "config.world_scaling = 0.5\n",
    "\n",
    "action_class = ContinuousAction(direction_type=\"steering\", relative=False)\n",
    "env = WebotsEnv(train=True, \n",
    "                action_class=action_class, \n",
    "                evaluate_class=MyEval,\n",
    "                observation_class=MyObs,\n",
    "                config=config)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(100000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done is True:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
