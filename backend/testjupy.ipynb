{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import environment\n",
    "import importlib\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n",
      "sending: env\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(environment)\n",
    "env = environment.WebotsEnv(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(0, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "********** Iteration 0 ************\n",
      "sending: reset\n",
      "Reward ( 10 )\t 89.43792400715714\n",
      "Reward ( 20 )\t 89.37378496616735\n",
      "Reward ( 30 )\t 89.30568848835622\n",
      "Reward ( 40 )\t 89.24621604337773\n",
      "Reward ( 50 )\t 89.26176283117671\n",
      "Reward ( 60 )\t 89.22480875468759\n",
      "Reward ( 70 )\t 89.2297229910296\n",
      "Reward ( 80 )\t 89.23509140883775\n",
      "Reward ( 90 )\t 89.41926808257955\n",
      "Reward ( 100 )\t 89.41926808257955\n",
      "Reward ( 110 )\t 89.5166726030988\n",
      "Reward ( 120 )\t 89.65455945774772\n",
      "Reward ( 130 )\t 89.65343931915444\n",
      "Reward ( 140 )\t 89.63752570754556\n",
      "Reward ( 150 )\t 89.63943032102219\n",
      "Reward ( 160 )\t 89.6624239217991\n",
      "Reward ( 170 )\t 89.6624239217991\n",
      "Reward ( 180 )\t 89.71081460289464\n",
      "Reward ( 190 )\t 89.7067330403264\n",
      "Reward ( 200 )\t 89.68070257797964\n",
      "Reward ( 210 )\t 89.69772783126531\n",
      "Reward ( 220 )\t 89.70143542863772\n",
      "Reward ( 230 )\t 89.72923135232477\n",
      "Reward ( 240 )\t 89.68627942382128\n",
      "Reward ( 250 )\t 89.68360637659909\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00379 |      -0.02196 |      2.05e+06 |       0.00115 |       2.19598\n",
      "     -0.01976 |      -0.02188 |      2.04e+06 |       0.00855 |       2.18847\n",
      "     -0.02575 |      -0.02180 |      2.04e+06 |       0.01718 |       2.17985\n",
      "     -0.02845 |      -0.02185 |      2.04e+06 |       0.01212 |       2.18491\n",
      "Evaluating losses...\n",
      "     -0.02828 |      -0.02188 |      2.04e+06 |       0.00950 |       2.18754\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 11           |\n",
      "| TimestepsSoFar  | 256          |\n",
      "| ev_tdlam_before | -0.000416    |\n",
      "| loss_ent        | 2.1875377    |\n",
      "| loss_kl         | 0.009501257  |\n",
      "| loss_pol_entpen | -0.021875378 |\n",
      "| loss_pol_surr   | -0.02828358  |\n",
      "| loss_vf_loss    | 2036636.2    |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Reward ( 260 )\t 89.6514062944091\n",
      "Reward ( 270 )\t 89.66072848089318\n",
      "Reward ( 280 )\t 89.69369975678524\n",
      "Reward ( 290 )\t 89.68589815388148\n",
      "Reward ( 300 )\t 89.66540922332274\n",
      "Reward ( 310 )\t 89.68984888320185\n",
      "Reward ( 320 )\t 89.62687342550922\n",
      "Reward ( 330 )\t 89.62687342550922\n",
      "Reward ( 340 )\t 89.60375626599999\n",
      "Reward ( 350 )\t 89.65897843183133\n",
      "Reward ( 360 )\t 89.7059051754087\n",
      "Reward ( 370 )\t 89.7171830209145\n",
      "Reward ( 380 )\t 89.7632036347561\n",
      "Reward ( 390 )\t 89.77219175992873\n",
      "Reward ( 400 )\t 89.69472254081442\n",
      "Reward ( 410 )\t 89.71921845617962\n",
      "Reward ( 420 )\t 89.68492393374899\n",
      "Reward ( 430 )\t 89.7183752108951\n",
      "Reward ( 440 )\t 89.69131577987795\n",
      "Reward ( 450 )\t 89.64230582411417\n",
      "Reward ( 460 )\t 89.69038214811715\n",
      "Reward ( 470 )\t 89.68665606066662\n",
      "Reward ( 480 )\t 89.67371416843567\n",
      "Reward ( 490 )\t 89.70960849069218\n",
      "Reward ( 500 )\t 89.69368872949757\n",
      "Reward ( 510 )\t 89.62670285038577\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00032 |      -0.02187 |      2.06e+06 |       0.00022 |       2.18679\n",
      "     -0.01308 |      -0.02180 |      2.05e+06 |       0.00239 |       2.17979\n",
      "     -0.02335 |      -0.02166 |      2.05e+06 |       0.00979 |       2.16617\n",
      "     -0.02447 |      -0.02157 |      2.05e+06 |       0.01780 |       2.15674\n",
      "Evaluating losses...\n",
      "     -0.02614 |      -0.02157 |      2.05e+06 |       0.01963 |       2.15674\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 19.5         |\n",
      "| TimestepsSoFar  | 512          |\n",
      "| ev_tdlam_before | 1.84e-05     |\n",
      "| loss_ent        | 2.1567407    |\n",
      "| loss_kl         | 0.019633677  |\n",
      "| loss_pol_entpen | -0.021567406 |\n",
      "| loss_pol_surr   | -0.026143914 |\n",
      "| loss_vf_loss    | 2051870.6    |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Reward ( 520 )\t 89.64697462169731\n",
      "Reward ( 530 )\t 89.77971085366539\n",
      "Reward ( 540 )\t 89.70873675434557\n",
      "Reward ( 550 )\t 89.61932754869926\n",
      "Reward ( 560 )\t 89.61712472587988\n",
      "Reward ( 570 )\t 89.61487803986155\n",
      "Reward ( 580 )\t 89.58032126011895\n",
      "Reward ( 590 )\t 89.71626597587698\n",
      "Reward ( 600 )\t 89.79073557443246\n",
      "Reward ( 610 )\t 89.77316712480051\n",
      "Reward ( 620 )\t 89.7415640464261\n",
      "Reward ( 630 )\t 89.7952456604176\n",
      "Reward ( 640 )\t 89.78915251047022\n",
      "Reward ( 650 )\t 89.75512482561442\n",
      "Reward ( 660 )\t 89.7653030423896\n",
      "Reward ( 670 )\t 89.73211856785679\n",
      "Reward ( 680 )\t 89.70909157316763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward ( 690 )\t 89.63036265509272\n",
      "Reward ( 700 )\t 89.56478423756776\n",
      "Reward ( 710 )\t 89.56478423756776\n",
      "Reward ( 720 )\t 89.56097251202122\n",
      "Reward ( 730 )\t 89.61517223769297\n",
      "Reward ( 740 )\t 89.59664786615707\n",
      "Reward ( 750 )\t 89.62510522510631\n",
      "Reward ( 760 )\t 89.62469802516691\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00267 |      -0.02158 |      2.06e+06 |       0.00025 |       2.15808\n",
      "     -0.00125 |      -0.02155 |      2.05e+06 |       0.00148 |       2.15518\n",
      "     -0.01096 |      -0.02148 |      2.05e+06 |       0.00453 |       2.14843\n",
      "     -0.01796 |      -0.02141 |      2.05e+06 |       0.01061 |       2.14097\n",
      "Evaluating losses...\n",
      "     -0.02033 |      -0.02137 |      2.05e+06 |       0.01515 |       2.13664\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 31           |\n",
      "| TimestepsSoFar  | 768          |\n",
      "| ev_tdlam_before | 1.87e-05     |\n",
      "| loss_ent        | 2.1366377    |\n",
      "| loss_kl         | 0.015149197  |\n",
      "| loss_pol_entpen | -0.021366376 |\n",
      "| loss_pol_surr   | -0.020330124 |\n",
      "| loss_vf_loss    | 2052230.5    |\n",
      "----------------------------------\n",
      "********** Iteration 3 ************\n",
      "Reward ( 770 )\t 89.64821194339807\n",
      "Reward ( 780 )\t 89.60149137225702\n",
      "Reward ( 790 )\t 89.62307956396123\n",
      "Reward ( 800 )\t 89.61106949968689\n",
      "Reward ( 810 )\t 89.59549965601656\n",
      "Reward ( 820 )\t 89.55792921291217\n",
      "Reward ( 830 )\t 89.55912692677875\n",
      "Reward ( 840 )\t 89.55529899632081\n",
      "Reward ( 850 )\t 89.67241709086437\n",
      "Reward ( 860 )\t 89.70472886639534\n",
      "Reward ( 870 )\t 89.6552602377327\n",
      "Reward ( 880 )\t 89.6639331662499\n",
      "Reward ( 890 )\t 89.66633478939353\n",
      "Reward ( 900 )\t 89.66603773672958\n",
      "Reward ( 910 )\t 89.66677262097937\n",
      "Reward ( 920 )\t 89.69093376940519\n",
      "Reward ( 930 )\t 89.70976356103507\n",
      "Reward ( 940 )\t 89.71081945982672\n",
      "Reward ( 950 )\t 89.71966316959339\n",
      "Reward ( 960 )\t 89.71267841671107\n",
      "Reward ( 970 )\t 89.7284820913898\n",
      "Reward ( 980 )\t 89.71663664882807\n",
      "Reward ( 990 )\t 89.72335252979458\n",
      "Reward ( 1000 )\t 89.72021634182349\n",
      "Reward ( 1010 )\t 89.74736113674652\n",
      "Reward ( 1020 )\t 89.77394092891012\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00140 |      -0.02140 |      2.05e+06 |       0.00022 |       2.13989\n",
      "     -0.01299 |      -0.02155 |      2.05e+06 |       0.00304 |       2.15474\n",
      "     -0.02417 |      -0.02166 |      2.05e+06 |       0.01159 |       2.16639\n",
      "     -0.02820 |      -0.02162 |      2.05e+06 |       0.02204 |       2.16238\n",
      "Evaluating losses...\n",
      "     -0.02825 |      -0.02157 |      2.05e+06 |       0.02720 |       2.15672\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 38.8         |\n",
      "| TimestepsSoFar  | 1024         |\n",
      "| ev_tdlam_before | 1.73e-06     |\n",
      "| loss_ent        | 2.1567245    |\n",
      "| loss_kl         | 0.027201336  |\n",
      "| loss_pol_entpen | -0.021567244 |\n",
      "| loss_pol_surr   | -0.02824518  |\n",
      "| loss_vf_loss    | 2051280.6    |\n",
      "----------------------------------\n",
      "********** Iteration 4 ************\n",
      "Reward ( 1030 )\t 89.77726191156358\n",
      "Reward ( 1040 )\t 89.77726191156358\n",
      "Reward ( 1050 )\t 89.77067167264076\n",
      "Reward ( 1060 )\t 89.74014713004227\n",
      "Reward ( 1070 )\t 89.7232757053706\n",
      "Reward ( 1080 )\t 89.7071317166267\n",
      "Reward ( 1090 )\t 89.7071317166267\n",
      "Reward ( 1100 )\t 89.71054197605373\n",
      "Reward ( 1110 )\t 89.72150890243361\n",
      "Reward ( 1120 )\t 89.71769375619245\n",
      "Reward ( 1130 )\t 89.70284363574923\n",
      "Reward ( 1140 )\t 89.68173909948297\n",
      "Reward ( 1150 )\t 89.62703845869915\n",
      "Reward ( 1160 )\t 89.6217731448398\n",
      "Reward ( 1170 )\t 89.59279851577782\n",
      "Reward ( 1180 )\t 89.5796793104982\n",
      "Reward ( 1190 )\t 89.566998196563\n",
      "Reward ( 1200 )\t 89.55778972338126\n",
      "Reward ( 1210 )\t 89.54501334945873\n",
      "Reward ( 1220 )\t 89.54495229143298\n",
      "Reward ( 1230 )\t 89.5015506802312\n",
      "Reward ( 1240 )\t 89.50070389111087\n",
      "Reward ( 1250 )\t 89.50828737881034\n",
      "Reward ( 1260 )\t 89.5089887149762\n",
      "Reward ( 1270 )\t 89.52288524674591\n",
      "Reward ( 1280 )\t 89.53487828503793\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00031 |      -0.02158 |      2.05e+06 |       0.00012 |       2.15752\n",
      "     -0.00385 |      -0.02160 |      2.05e+06 |       0.00128 |       2.16001\n",
      "     -0.01103 |      -0.02168 |      2.05e+06 |       0.00607 |       2.16765\n",
      "     -0.01792 |      -0.02170 |      2.05e+06 |       0.01407 |       2.17020\n",
      "Evaluating losses...\n",
      "     -0.01984 |      -0.02171 |      2.05e+06 |       0.01859 |       2.17061\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 45           |\n",
      "| TimestepsSoFar  | 1280         |\n",
      "| ev_tdlam_before | 5.25e-06     |\n",
      "| loss_ent        | 2.1706114    |\n",
      "| loss_kl         | 0.018585868  |\n",
      "| loss_pol_entpen | -0.021706112 |\n",
      "| loss_pol_surr   | -0.019838318 |\n",
      "| loss_vf_loss    | 2049026.5    |\n",
      "----------------------------------\n",
      "********** Iteration 5 ************\n",
      "Reward ( 1290 )\t 89.51815628517325\n",
      "Reward ( 1300 )\t 89.4981796783147\n",
      "Reward ( 1310 )\t 89.49852450862761\n",
      "Reward ( 1320 )\t 89.49957066847571\n",
      "Reward ( 1330 )\t 89.48653144871338\n",
      "Reward ( 1340 )\t 89.49151106487132\n",
      "Reward ( 1350 )\t 89.48136890859583\n",
      "Reward ( 1360 )\t 89.47188492712823\n",
      "Reward ( 1370 )\t 89.47328514507403\n",
      "Reward ( 1380 )\t 89.46442430715473\n",
      "Reward ( 1390 )\t 89.46552258557068\n",
      "Reward ( 1400 )\t 89.48019142173092\n",
      "Reward ( 1410 )\t 89.47928530763309\n",
      "Reward ( 1420 )\t 89.47928530763309\n",
      "Reward ( 1430 )\t 89.47751261433932\n",
      "Reward ( 1440 )\t 89.46687146226498\n",
      "Reward ( 1450 )\t 89.47113047817618\n",
      "Reward ( 1460 )\t 89.47033093213689\n",
      "Reward ( 1470 )\t 89.47579167781268\n",
      "Reward ( 1480 )\t 89.44550675137542\n",
      "Reward ( 1490 )\t 89.44924220416902\n",
      "Reward ( 1500 )\t 89.44191219228014\n",
      "Reward ( 1510 )\t 89.43735540565274\n",
      "Reward ( 1520 )\t 89.4234834864879\n",
      "Reward ( 1530 )\t 89.45547436024391\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00019 |      -0.02166 |      2.04e+06 |       0.00028 |       2.16632\n",
      "     -0.01106 |      -0.02153 |      2.04e+06 |       0.00351 |       2.15328\n",
      "     -0.02031 |      -0.02130 |      2.04e+06 |       0.01405 |       2.13029\n",
      "     -0.02434 |      -0.02119 |      2.04e+06 |       0.02229 |       2.11919\n",
      "Evaluating losses...\n",
      "     -0.02642 |      -0.02121 |      2.04e+06 |       0.02380 |       2.12077\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 53.2         |\n",
      "| TimestepsSoFar  | 1536         |\n",
      "| ev_tdlam_before | 1.06e-05     |\n",
      "| loss_ent        | 2.120773     |\n",
      "| loss_kl         | 0.023801766  |\n",
      "| loss_pol_entpen | -0.021207731 |\n",
      "| loss_pol_surr   | -0.026417665 |\n",
      "| loss_vf_loss    | 2041365.8    |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Reward ( 1540 )\t 89.43913312554305\n",
      "Reward ( 1550 )\t 89.42466563148848\n",
      "Reward ( 1560 )\t 89.42341490732085\n",
      "Reward ( 1570 )\t 89.42720344360082\n",
      "Reward ( 1580 )\t 89.42720344360082\n",
      "Reward ( 1590 )\t 89.43570339668238\n",
      "Reward ( 1600 )\t 89.43214844873957\n",
      "Reward ( 1610 )\t 89.42451944614382\n",
      "Reward ( 1620 )\t 89.43271903830427\n",
      "Reward ( 1630 )\t 89.42682005385426\n",
      "Reward ( 1640 )\t 89.432079851518\n",
      "Reward ( 1650 )\t 89.41137429472809\n",
      "Reward ( 1660 )\t 89.4247627159137\n",
      "Reward ( 1670 )\t 89.42572742133481\n",
      "Reward ( 1680 )\t 89.4180825672536\n",
      "Reward ( 1690 )\t 89.41260097583458\n",
      "Reward ( 1700 )\t 89.41272239515156\n",
      "Reward ( 1710 )\t 89.4133660336433\n",
      "Reward ( 1720 )\t 89.41290203943207\n",
      "Reward ( 1730 )\t 89.41020863899791\n",
      "Reward ( 1740 )\t 89.4147280822297\n",
      "Reward ( 1750 )\t 89.42596313081296\n",
      "Reward ( 1760 )\t 89.39419683013597\n",
      "Reward ( 1770 )\t 89.39419683013597\n",
      "Reward ( 1780 )\t 89.39174076847006\n",
      "Reward ( 1790 )\t 89.37308933291102\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00088 |      -0.02122 |      2.04e+06 |       0.00035 |       2.12152\n",
      "     -0.01107 |      -0.02125 |      2.04e+06 |       0.00300 |       2.12462\n",
      "     -0.01858 |      -0.02126 |      2.04e+06 |       0.00975 |       2.12621\n",
      "     -0.02145 |      -0.02120 |      2.04e+06 |       0.02034 |       2.11978\n",
      "Evaluating losses...\n",
      "     -0.01982 |      -0.02115 |      2.04e+06 |       0.02554 |       2.11456\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 60           |\n",
      "| TimestepsSoFar  | 1792         |\n",
      "| ev_tdlam_before | -3.1e-06     |\n",
      "| loss_ent        | 2.114556     |\n",
      "| loss_kl         | 0.025542889  |\n",
      "| loss_pol_entpen | -0.02114556  |\n",
      "| loss_pol_surr   | -0.019820966 |\n",
      "| loss_vf_loss    | 2038435.8    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 7 ************\n",
      "Reward ( 1800 )\t 89.38755699827796\n",
      "Reward ( 1810 )\t 89.37665857035375\n",
      "Reward ( 1820 )\t 89.38379652420218\n",
      "Reward ( 1830 )\t 89.38554250802996\n",
      "Reward ( 1840 )\t 89.38169421863641\n",
      "Reward ( 1850 )\t 89.37140672792502\n",
      "Reward ( 1860 )\t 89.37846685468801\n",
      "Reward ( 1870 )\t 89.37020375338793\n",
      "Reward ( 1880 )\t 89.37020375338793\n",
      "Reward ( 1890 )\t 89.3692442472429\n",
      "Reward ( 1900 )\t 89.3601621618082\n",
      "Reward ( 1910 )\t 89.35183242714875\n",
      "Reward ( 1920 )\t 89.35183242714875\n",
      "Reward ( 1930 )\t 89.36362419521797\n",
      "Reward ( 1940 )\t 89.36964221200665\n",
      "Reward ( 1950 )\t 89.38085619072162\n",
      "Reward ( 1960 )\t 89.38225904911486\n",
      "Reward ( 1970 )\t 89.3862100077169\n",
      "Reward ( 1980 )\t 89.35247729233797\n",
      "Reward ( 1990 )\t 89.34792298972924\n",
      "Reward ( 2000 )\t 89.33695681857215\n",
      "Reward ( 2010 )\t 89.35032790198913\n",
      "Reward ( 2020 )\t 89.35912723198626\n",
      "Reward ( 2030 )\t 89.3406263473522\n",
      "Reward ( 2040 )\t 89.35788735658288\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00074 |      -0.02112 |      2.04e+06 |      9.26e-05 |       2.11166\n",
      "     -0.00658 |      -0.02099 |      2.04e+06 |       0.00150 |       2.09893\n",
      "     -0.01724 |      -0.02082 |      2.04e+06 |       0.00690 |       2.08250\n",
      "     -0.02146 |      -0.02062 |      2.04e+06 |       0.01643 |       2.06204\n",
      "Evaluating losses...\n",
      "     -0.01983 |      -0.02053 |      2.04e+06 |       0.02093 |       2.05339\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 66.8         |\n",
      "| TimestepsSoFar  | 2048         |\n",
      "| ev_tdlam_before | 9.95e-06     |\n",
      "| loss_ent        | 2.053389     |\n",
      "| loss_kl         | 0.020925913  |\n",
      "| loss_pol_entpen | -0.020533891 |\n",
      "| loss_pol_surr   | -0.019828375 |\n",
      "| loss_vf_loss    | 2035670.2    |\n",
      "----------------------------------\n",
      "********** Iteration 8 ************\n",
      "Reward ( 2050 )\t 89.42054091941174\n",
      "Reward ( 2060 )\t 89.41819060477305\n",
      "Reward ( 2070 )\t 89.41794439784738\n",
      "Reward ( 2080 )\t 89.41794439784738\n",
      "Reward ( 2090 )\t 89.41794439784738\n",
      "Reward ( 2100 )\t 89.41152181984266\n",
      "Reward ( 2110 )\t 89.39320581918103\n",
      "Reward ( 2120 )\t 89.31755644246282\n",
      "Reward ( 2130 )\t 89.34308553298813\n",
      "Reward ( 2140 )\t 89.34308553298813\n",
      "Reward ( 2150 )\t 89.36313220374514\n",
      "Reward ( 2160 )\t 89.39111959477061\n",
      "Reward ( 2170 )\t 89.4032576329925\n",
      "Reward ( 2180 )\t 89.42796934126945\n",
      "Reward ( 2190 )\t 89.42052494271414\n",
      "Reward ( 2200 )\t 89.41616002556658\n",
      "Reward ( 2210 )\t 89.36354889299027\n",
      "Reward ( 2220 )\t 89.38718226845198\n",
      "Reward ( 2230 )\t 89.35168242128522\n",
      "Reward ( 2240 )\t 89.41243285394758\n",
      "Reward ( 2250 )\t 89.41243285394758\n",
      "Reward ( 2260 )\t 89.51132115001455\n",
      "Reward ( 2270 )\t 89.53358846463765\n",
      "Reward ( 2280 )\t 89.54173137985383\n",
      "Reward ( 2290 )\t 89.54866931471844\n",
      "Reward ( 2300 )\t 89.5512773887442\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00076 |      -0.02052 |      2.04e+06 |       0.00020 |       2.05197\n",
      "     -0.00198 |      -0.02058 |      2.04e+06 |       0.00145 |       2.05766\n",
      "     -0.00871 |      -0.02068 |      2.04e+06 |       0.00433 |       2.06786\n",
      "     -0.01418 |      -0.02072 |      2.04e+06 |       0.00912 |       2.07162\n",
      "Evaluating losses...\n",
      "     -0.01507 |      -0.02070 |      2.04e+06 |       0.01235 |       2.07025\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 72.8         |\n",
      "| TimestepsSoFar  | 2304         |\n",
      "| ev_tdlam_before | 2.86e-06     |\n",
      "| loss_ent        | 2.0702538    |\n",
      "| loss_kl         | 0.01234917   |\n",
      "| loss_pol_entpen | -0.020702539 |\n",
      "| loss_pol_surr   | -0.01506727  |\n",
      "| loss_vf_loss    | 2037438.9    |\n",
      "----------------------------------\n",
      "********** Iteration 9 ************\n",
      "Reward ( 2310 )\t 89.5875699297446\n",
      "Reward ( 2320 )\t 89.60728468815987\n",
      "Reward ( 2330 )\t 89.64259027324464\n",
      "Reward ( 2340 )\t 89.70044443385743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-739dc19df018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    240\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"********** Iteration %i ************\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miters_so_far\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;31m# Stop training early (triggered by the callback)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mtraj_segment_generator\u001b[0;34m(policy, env, horizon, reward_giver, gail, callback)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ees-pees/backend/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ees-pees/backend/environment.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;34m\"\"\"Receive state via Com class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ees-pees/backend/communicate.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_sock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvfrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPACKET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_from_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_steps = 200000\n",
    "model_name = \"Webots1\"\n",
    "\n",
    "model = PPO1(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=time_steps, log_interval=100)\n",
    "model.save(\"models/{}\".format(model_name))\n",
    "\n",
    "# model = PPO1.load(\"models/{}\".format('DQN_WebotFakeMini_TRPO_pj1_nReward2_200000'))\n",
    "# env = MyEnv()\n",
    "# obs = env.reset()\n",
    "\n",
    "# env.render()\n",
    "# done = False\n",
    "# max_num_steps = 100\n",
    "# time = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
