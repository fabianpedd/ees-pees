{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import environment\n",
    "import importlib\n",
    "\n",
    "from evaluate import EvaluateMats\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n",
      "sending: env\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(environment)\n",
    "env = environment.WebotsEnv(train=True, evaluate_class=EvaluateMats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pj/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "********** Iteration 0 ************\n",
      "sending: reset\n",
      "Reward ( 10 )\t -11.222235465417574\n",
      "Reward ( 20 )\t -11.23654789473575\n",
      "Reward ( 30 )\t -11.248934500169492\n",
      "Reward ( 40 )\t -11.248934500169492\n",
      "Reward ( 50 )\t -11.25042584926658\n",
      "Reward ( 60 )\t -11.25042584926658\n",
      "Reward ( 70 )\t -11.252001963646759\n",
      "Reward ( 80 )\t -11.241682915672095\n",
      "Reward ( 90 )\t -11.240177531493684\n",
      "Reward ( 100 )\t -11.249205716433007\n",
      "Reward ( 110 )\t -11.257694974805025\n",
      "Reward ( 120 )\t -11.257694974805025\n",
      "Reward ( 130 )\t -11.26458579850978\n",
      "Reward ( 140 )\t -11.271481851048494\n",
      "Reward ( 150 )\t -11.277451232514403\n",
      "Reward ( 160 )\t -11.286797752478817\n",
      "Reward ( 170 )\t -11.267923069708319\n",
      "Reward ( 180 )\t -11.267923069708319\n",
      "Reward ( 190 )\t -11.261462422670904\n",
      "Reward ( 200 )\t -11.278504458919814\n",
      "Reward ( 210 )\t -11.296645813715639\n",
      "Reward ( 220 )\t -11.295723668475489\n",
      "Reward ( 230 )\t -11.332278088633739\n",
      "Reward ( 240 )\t -11.316595023678508\n",
      "Reward ( 250 )\t -11.332194347674285\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00493 |      -0.02196 |      3.20e+04 |       0.00100 |       2.19620\n",
      "     -0.00965 |      -0.02195 |      3.13e+04 |       0.00279 |       2.19456\n",
      "     -0.01763 |      -0.02191 |      3.09e+04 |       0.00679 |       2.19074\n",
      "     -0.02570 |      -0.02188 |      3.07e+04 |       0.00945 |       2.18822\n",
      "Evaluating losses...\n",
      "     -0.03117 |      -0.02188 |      3.06e+04 |       0.01016 |       2.18755\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 12.1         |\n",
      "| TimestepsSoFar  | 256          |\n",
      "| ev_tdlam_before | -0.00979     |\n",
      "| loss_ent        | 2.1875463    |\n",
      "| loss_kl         | 0.010155054  |\n",
      "| loss_pol_entpen | -0.021875462 |\n",
      "| loss_pol_surr   | -0.031169526 |\n",
      "| loss_vf_loss    | 30605.004    |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Reward ( 260 )\t -11.368000666339114\n",
      "Reward ( 270 )\t -11.395995433739794\n",
      "Reward ( 280 )\t -11.402603446762617\n",
      "Reward ( 290 )\t -11.431916717022286\n",
      "Reward ( 300 )\t -11.409536359967898\n",
      "Reward ( 310 )\t -11.354873883054564\n",
      "Reward ( 320 )\t -11.294284101337634\n",
      "Reward ( 330 )\t -11.290041658666787\n",
      "Reward ( 340 )\t -11.281114670827549\n",
      "Reward ( 350 )\t -11.278922395465282\n",
      "Reward ( 360 )\t -11.269575600753647\n",
      "Reward ( 370 )\t -11.280710167568143\n",
      "Reward ( 380 )\t -11.267371143191955\n",
      "Reward ( 390 )\t -11.260700777953899\n",
      "Reward ( 400 )\t -11.252177284130202\n",
      "Reward ( 410 )\t -11.246884736191074\n",
      "Reward ( 420 )\t -11.249285031947194\n",
      "Reward ( 430 )\t -11.239799368346604\n",
      "Reward ( 440 )\t -11.261085898038342\n",
      "Reward ( 450 )\t -11.258780196293403\n",
      "Reward ( 460 )\t -11.257145770382255\n",
      "Reward ( 470 )\t -11.22334426581963\n",
      "Reward ( 480 )\t -11.180212384198365\n",
      "Reward ( 490 )\t -11.176269623521616\n",
      "Reward ( 500 )\t -11.175265876603238\n",
      "Reward ( 510 )\t -11.168008574699787\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00070 |      -0.02189 |      3.22e+04 |       0.00028 |       2.18917\n",
      "     -0.01227 |      -0.02186 |      3.20e+04 |       0.00238 |       2.18637\n",
      "     -0.02208 |      -0.02181 |      3.19e+04 |       0.00841 |       2.18074\n",
      "     -0.02383 |      -0.02172 |      3.17e+04 |       0.01552 |       2.17226\n",
      "Evaluating losses...\n",
      "     -0.02727 |      -0.02169 |      3.17e+04 |       0.01637 |       2.16945\n",
      "---------------------------------\n",
      "| EpThisIter      | 0           |\n",
      "| EpisodesSoFar   | 0           |\n",
      "| TimeElapsed     | 21.4        |\n",
      "| TimestepsSoFar  | 512         |\n",
      "| ev_tdlam_before | 0.000287    |\n",
      "| loss_ent        | 2.1694503   |\n",
      "| loss_kl         | 0.016367387 |\n",
      "| loss_pol_entpen | -0.0216945  |\n",
      "| loss_pol_surr   | -0.02726689 |\n",
      "| loss_vf_loss    | 31667.238   |\n",
      "---------------------------------\n",
      "********** Iteration 2 ************\n",
      "Reward ( 520 )\t -11.17703796282673\n",
      "Reward ( 530 )\t -11.176150175878302\n",
      "Reward ( 540 )\t -11.170562449379041\n",
      "Reward ( 550 )\t -11.173829012844426\n",
      "Reward ( 560 )\t -11.175685056034235\n",
      "Reward ( 570 )\t -11.189845519063153\n",
      "Reward ( 580 )\t -11.189090389599986\n",
      "Reward ( 590 )\t -11.16079785176417\n",
      "Reward ( 600 )\t -11.14048459940686\n",
      "Reward ( 610 )\t -11.140937922844639\n",
      "Reward ( 620 )\t -11.146536992719366\n",
      "Reward ( 630 )\t -11.148930077782765\n",
      "Reward ( 640 )\t -11.123429327205027\n",
      "Reward ( 650 )\t -11.135027910157133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward ( 660 )\t -11.147122681746211\n",
      "Reward ( 670 )\t -11.14670195001668\n",
      "Reward ( 680 )\t -11.142988402061055\n",
      "Reward ( 690 )\t -11.14788308467701\n",
      "Reward ( 700 )\t -11.149201323786611\n",
      "Reward ( 710 )\t -11.153259572244222\n",
      "Reward ( 720 )\t -11.157598961141723\n",
      "Reward ( 730 )\t -11.150883052363856\n",
      "Reward ( 740 )\t -11.147748422362373\n",
      "Reward ( 750 )\t -11.149687670430438\n",
      "Reward ( 760 )\t -11.150666358998505\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00043 |      -0.02167 |      3.14e+04 |       0.00031 |       2.16655\n",
      "     -0.01017 |      -0.02158 |      3.13e+04 |       0.00283 |       2.15831\n",
      "     -0.01742 |      -0.02148 |      3.12e+04 |       0.00739 |       2.14810\n",
      "     -0.02193 |      -0.02137 |      3.11e+04 |       0.01361 |       2.13693\n",
      "Evaluating losses...\n",
      "     -0.02519 |      -0.02132 |      3.10e+04 |       0.01742 |       2.13217\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 29.9         |\n",
      "| TimestepsSoFar  | 768          |\n",
      "| ev_tdlam_before | 7.53e-05     |\n",
      "| loss_ent        | 2.1321735    |\n",
      "| loss_kl         | 0.017415617  |\n",
      "| loss_pol_entpen | -0.021321736 |\n",
      "| loss_pol_surr   | -0.02518599  |\n",
      "| loss_vf_loss    | 30999.55     |\n",
      "----------------------------------\n",
      "********** Iteration 3 ************\n",
      "Reward ( 770 )\t -11.153206746978972\n",
      "Reward ( 780 )\t -11.159725349694265\n",
      "Reward ( 790 )\t -11.171370689178877\n",
      "Reward ( 800 )\t -11.171370689178877\n",
      "Reward ( 810 )\t -11.171370689178877\n",
      "Reward ( 820 )\t -11.18422397458256\n",
      "Reward ( 830 )\t -11.194677590235516\n",
      "Reward ( 840 )\t -11.202866369341544\n",
      "Reward ( 850 )\t -11.216368309907876\n",
      "Reward ( 860 )\t -11.220491482795735\n",
      "Reward ( 870 )\t -11.226102024795933\n",
      "Reward ( 880 )\t -11.22466931471795\n",
      "Reward ( 890 )\t -11.223064175048425\n",
      "Reward ( 900 )\t -11.225455489012031\n",
      "Reward ( 910 )\t -11.23509830722233\n",
      "Reward ( 920 )\t -11.220645272208655\n",
      "Reward ( 930 )\t -11.213699382043986\n",
      "Reward ( 940 )\t -11.206516928165481\n",
      "Reward ( 950 )\t -11.212476864328984\n",
      "Reward ( 960 )\t -11.150116308609222\n",
      "Reward ( 970 )\t -11.10012290400772\n",
      "Reward ( 980 )\t -11.106614603871975\n",
      "Reward ( 990 )\t -11.112238838721787\n",
      "Reward ( 1000 )\t -11.116136335272834\n",
      "Reward ( 1010 )\t -11.112278941526808\n",
      "Reward ( 1020 )\t -11.112689895150952\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00028 |      -0.02136 |      3.15e+04 |       0.00024 |       2.13640\n",
      "     -0.00745 |      -0.02148 |      3.14e+04 |       0.00211 |       2.14818\n",
      "     -0.01451 |      -0.02163 |      3.13e+04 |       0.00783 |       2.16263\n",
      "     -0.01556 |      -0.02171 |      3.12e+04 |       0.01418 |       2.17105\n",
      "Evaluating losses...\n",
      "     -0.01649 |      -0.02173 |      3.11e+04 |       0.01568 |       2.17282\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 38.7         |\n",
      "| TimestepsSoFar  | 1024         |\n",
      "| ev_tdlam_before | -3.73e-05    |\n",
      "| loss_ent        | 2.1728208    |\n",
      "| loss_kl         | 0.015675647  |\n",
      "| loss_pol_entpen | -0.021728206 |\n",
      "| loss_pol_surr   | -0.016491737 |\n",
      "| loss_vf_loss    | 31122.2      |\n",
      "----------------------------------\n",
      "********** Iteration 4 ************\n",
      "Reward ( 1030 )\t -11.122221985167279\n",
      "Reward ( 1040 )\t -11.135852533219463\n",
      "Reward ( 1050 )\t -11.149503159126468\n",
      "Reward ( 1060 )\t -11.152841576563949\n",
      "Reward ( 1070 )\t -11.152404996857237\n",
      "Reward ( 1080 )\t -11.174632238907455\n",
      "Reward ( 1090 )\t -11.179308828289724\n",
      "Reward ( 1100 )\t -11.184228239391274\n",
      "Reward ( 1110 )\t -11.182083230180737\n",
      "Reward ( 1120 )\t -11.191342761050565\n",
      "Reward ( 1130 )\t -11.19238612489192\n",
      "Reward ( 1140 )\t -11.184785057762378\n",
      "Reward ( 1150 )\t -11.145314504780233\n",
      "Reward ( 1160 )\t -11.125552641837615\n",
      "Reward ( 1170 )\t -11.11305522459465\n",
      "Reward ( 1180 )\t -11.113732946130217\n",
      "Reward ( 1190 )\t -11.118990179403724\n",
      "Reward ( 1200 )\t -11.122811448677078\n",
      "Reward ( 1210 )\t -11.134326297394864\n",
      "Reward ( 1220 )\t -11.138074233207458\n",
      "Reward ( 1230 )\t -11.145793466015773\n",
      "Reward ( 1240 )\t -11.157496369646164\n",
      "Reward ( 1250 )\t -11.153033312396097\n",
      "Reward ( 1260 )\t -11.15719342556801\n",
      "Reward ( 1270 )\t -11.157048650392284\n",
      "Reward ( 1280 )\t -11.141403634481685\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00040 |      -0.02171 |      3.13e+04 |       0.00017 |       2.17084\n",
      "     -0.00672 |      -0.02164 |      3.12e+04 |       0.00165 |       2.16366\n",
      "     -0.01649 |      -0.02153 |      3.11e+04 |       0.00528 |       2.15332\n",
      "     -0.02330 |      -0.02148 |      3.10e+04 |       0.00989 |       2.14764\n",
      "Evaluating losses...\n",
      "     -0.02761 |      -0.02143 |      3.09e+04 |       0.01407 |       2.14332\n",
      "---------------------------------\n",
      "| EpThisIter      | 0           |\n",
      "| EpisodesSoFar   | 0           |\n",
      "| TimeElapsed     | 49.5        |\n",
      "| TimestepsSoFar  | 1280        |\n",
      "| ev_tdlam_before | 7.45e-06    |\n",
      "| loss_ent        | 2.143315    |\n",
      "| loss_kl         | 0.014074162 |\n",
      "| loss_pol_entpen | -0.02143315 |\n",
      "| loss_pol_surr   | -0.02761336 |\n",
      "| loss_vf_loss    | 30895.018   |\n",
      "---------------------------------\n",
      "********** Iteration 5 ************\n",
      "Reward ( 1290 )\t -11.117108954643601\n",
      "Reward ( 1300 )\t -11.135146882506195\n",
      "Reward ( 1310 )\t -11.131621280929727\n",
      "Reward ( 1320 )\t -11.131621280929727\n",
      "Reward ( 1330 )\t -11.130749227583113\n",
      "Reward ( 1340 )\t -11.14572062365983\n",
      "Reward ( 1350 )\t -11.149572807517421\n",
      "Reward ( 1360 )\t -11.149316906882738\n",
      "Reward ( 1370 )\t -11.137829465741419\n",
      "Reward ( 1380 )\t -11.135895985481273\n",
      "Reward ( 1390 )\t -11.139211282495632\n",
      "Reward ( 1400 )\t -11.142640857221096\n",
      "Reward ( 1410 )\t -11.137273851856277\n",
      "Reward ( 1420 )\t -11.12257604470456\n",
      "Reward ( 1430 )\t -11.125545423486775\n",
      "Reward ( 1440 )\t -11.11596886594133\n",
      "Reward ( 1450 )\t -11.092531307477959\n",
      "Reward ( 1460 )\t -11.068940679212197\n",
      "Reward ( 1470 )\t -11.0755360093877\n",
      "Reward ( 1480 )\t -11.085004108047327\n",
      "Reward ( 1490 )\t -11.095294736441526\n",
      "Reward ( 1500 )\t -11.099084707030945\n",
      "Reward ( 1510 )\t -11.111116483771367\n",
      "Reward ( 1520 )\t -11.113939654807616\n",
      "Reward ( 1530 )\t -11.127393188361456\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00074 |      -0.02139 |      3.10e+04 |       0.00036 |       2.13912\n",
      "     -0.00719 |      -0.02133 |      3.09e+04 |       0.00309 |       2.13313\n",
      "     -0.01517 |      -0.02125 |      3.08e+04 |       0.00915 |       2.12525\n",
      "     -0.01486 |      -0.02117 |      3.07e+04 |       0.01659 |       2.11723\n",
      "Evaluating losses...\n",
      "     -0.01516 |      -0.02118 |      3.07e+04 |       0.01867 |       2.11753\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 58.5         |\n",
      "| TimestepsSoFar  | 1536         |\n",
      "| ev_tdlam_before | -1.34e-05    |\n",
      "| loss_ent        | 2.1175327    |\n",
      "| loss_kl         | 0.018666716  |\n",
      "| loss_pol_entpen | -0.021175329 |\n",
      "| loss_pol_surr   | -0.01516143  |\n",
      "| loss_vf_loss    | 30664.996    |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Reward ( 1540 )\t -11.15932348092688\n",
      "Reward ( 1550 )\t -11.15932348092688\n",
      "Reward ( 1560 )\t -11.15932348092688\n",
      "Reward ( 1570 )\t -11.160003377756045\n",
      "Reward ( 1580 )\t -11.17290809674381\n",
      "Reward ( 1590 )\t -11.181201038886504\n",
      "Reward ( 1600 )\t -11.189537039521499\n",
      "Reward ( 1610 )\t -11.175490401423863\n",
      "Reward ( 1620 )\t -11.190419379630537\n",
      "Reward ( 1630 )\t -11.18528665477293\n",
      "Reward ( 1640 )\t -11.201571299573905\n",
      "Reward ( 1650 )\t -11.197193835901963\n",
      "Reward ( 1660 )\t -11.175174216134415\n",
      "Reward ( 1670 )\t -11.158532698608902\n",
      "Reward ( 1680 )\t -11.109654310794808\n",
      "Reward ( 1690 )\t -11.091503642526362\n",
      "Reward ( 1700 )\t -11.091031831531648\n",
      "Reward ( 1710 )\t -11.10417290488763\n",
      "Reward ( 1720 )\t -11.118603127612028\n",
      "Reward ( 1730 )\t -11.113178889086813\n",
      "Reward ( 1740 )\t -11.111454997149572\n",
      "Reward ( 1750 )\t -11.119636170667722\n",
      "Reward ( 1760 )\t -11.134238297012821\n",
      "Reward ( 1770 )\t -11.149618927873345\n",
      "Reward ( 1780 )\t -11.167204609705715\n",
      "Reward ( 1790 )\t -11.185491261791428\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00122 |      -0.02120 |      3.11e+04 |       0.00020 |       2.11994\n",
      "     -0.01535 |      -0.02121 |      3.10e+04 |       0.00306 |       2.12053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.02744 |      -0.02113 |      3.09e+04 |       0.01169 |       2.11340\n",
      "     -0.02655 |      -0.02112 |      3.08e+04 |       0.02043 |       2.11208\n",
      "Evaluating losses...\n",
      "     -0.02770 |      -0.02114 |      3.08e+04 |       0.02437 |       2.11362\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 68.2         |\n",
      "| TimestepsSoFar  | 1792         |\n",
      "| ev_tdlam_before | 7.03e-06     |\n",
      "| loss_ent        | 2.1136193    |\n",
      "| loss_kl         | 0.024369325  |\n",
      "| loss_pol_entpen | -0.021136193 |\n",
      "| loss_pol_surr   | -0.027702844 |\n",
      "| loss_vf_loss    | 30776.918    |\n",
      "----------------------------------\n",
      "********** Iteration 7 ************\n",
      "Reward ( 1800 )\t -11.193110408894777\n",
      "Reward ( 1810 )\t -11.19050289642009\n",
      "Reward ( 1820 )\t -11.213728989536508\n",
      "Reward ( 1830 )\t -11.233579123472294\n",
      "Reward ( 1840 )\t -11.234008921012277\n",
      "Reward ( 1850 )\t -11.23723124111689\n",
      "Reward ( 1860 )\t -11.235877166103894\n",
      "Reward ( 1870 )\t -11.241144949585058\n",
      "Reward ( 1880 )\t -11.248270793426629\n",
      "Reward ( 1890 )\t -11.253743151985438\n",
      "Reward ( 1900 )\t -11.255134727898017\n",
      "Reward ( 1910 )\t -11.255905445258493\n",
      "Reward ( 1920 )\t -11.261960428416554\n",
      "Reward ( 1930 )\t -11.273408072138613\n",
      "Reward ( 1940 )\t -11.281924501066142\n",
      "Reward ( 1950 )\t -11.305842086932525\n",
      "Reward ( 1960 )\t -11.307208168142587\n",
      "Reward ( 1970 )\t -11.310581168979956\n",
      "Reward ( 1980 )\t -11.327439050143056\n",
      "Reward ( 1990 )\t -11.32753804155076\n",
      "Reward ( 2000 )\t -11.323892065004186\n",
      "Reward ( 2010 )\t -11.320811472890773\n",
      "Reward ( 2020 )\t -11.323612409920093\n",
      "Reward ( 2030 )\t -11.327265748456629\n",
      "Reward ( 2040 )\t -11.330612136617507\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00036 |      -0.02112 |      3.18e+04 |       0.00028 |       2.11189\n",
      "     -0.00652 |      -0.02105 |      3.17e+04 |       0.00265 |       2.10517\n",
      "     -0.01918 |      -0.02090 |      3.16e+04 |       0.01007 |       2.08976\n",
      "     -0.02701 |      -0.02070 |      3.15e+04 |       0.02216 |       2.07003\n",
      "Evaluating losses...\n",
      "     -0.02820 |      -0.02061 |      3.14e+04 |       0.02936 |       2.06100\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 80.5         |\n",
      "| TimestepsSoFar  | 2048         |\n",
      "| ev_tdlam_before | 0.00018      |\n",
      "| loss_ent        | 2.060999     |\n",
      "| loss_kl         | 0.029360918  |\n",
      "| loss_pol_entpen | -0.02060999  |\n",
      "| loss_pol_surr   | -0.028196294 |\n",
      "| loss_vf_loss    | 31415.41     |\n",
      "----------------------------------\n",
      "********** Iteration 8 ************\n",
      "Reward ( 2050 )\t -11.333322208632056\n",
      "Reward ( 2060 )\t -11.349648667211975\n",
      "Reward ( 2070 )\t -11.340190741087039\n",
      "Reward ( 2080 )\t -11.351601468573318\n",
      "Reward ( 2090 )\t -11.344670983119068\n",
      "Reward ( 2100 )\t -11.346182859059391\n",
      "Reward ( 2110 )\t -11.356290561324435\n",
      "Reward ( 2120 )\t -11.364082909889747\n",
      "Reward ( 2130 )\t -11.361053092313075\n",
      "Reward ( 2140 )\t -11.37234033522984\n",
      "Reward ( 2150 )\t -11.37234033522984\n",
      "Reward ( 2160 )\t -11.366058074989777\n",
      "Reward ( 2170 )\t -11.368471076085836\n",
      "Reward ( 2180 )\t -11.357458215980278\n",
      "Reward ( 2190 )\t -11.34778156396391\n",
      "Reward ( 2200 )\t -11.347738564061444\n",
      "Reward ( 2210 )\t -11.374469292386848\n",
      "Reward ( 2220 )\t -11.387493135330077\n",
      "Reward ( 2230 )\t -11.39324908908208\n",
      "Reward ( 2240 )\t -11.393662773407042\n",
      "Reward ( 2250 )\t -11.386906379744069\n",
      "Reward ( 2260 )\t -11.390016575203287\n",
      "Reward ( 2270 )\t -11.386959974516067\n",
      "Reward ( 2280 )\t -11.385949278707372\n",
      "Reward ( 2290 )\t -11.382902353344214\n",
      "Reward ( 2300 )\t -11.375507196727092\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00184 |      -0.02067 |      3.22e+04 |       0.00039 |       2.06744\n",
      "     -0.00585 |      -0.02084 |      3.21e+04 |       0.00259 |       2.08371\n",
      "     -0.00823 |      -0.02096 |      3.20e+04 |       0.00559 |       2.09574\n",
      "     -0.01279 |      -0.02101 |      3.19e+04 |       0.00774 |       2.10076\n",
      "Evaluating losses...\n",
      "     -0.01618 |      -0.02101 |      3.19e+04 |       0.00889 |       2.10123\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 91.6         |\n",
      "| TimestepsSoFar  | 2304         |\n",
      "| ev_tdlam_before | 0.000742     |\n",
      "| loss_ent        | 2.101234     |\n",
      "| loss_kl         | 0.008887034  |\n",
      "| loss_pol_entpen | -0.021012342 |\n",
      "| loss_pol_surr   | -0.01618142  |\n",
      "| loss_vf_loss    | 31881.662    |\n",
      "----------------------------------\n",
      "********** Iteration 9 ************\n",
      "Reward ( 2310 )\t -11.37436449999965\n",
      "Reward ( 2320 )\t -11.374146892815919\n",
      "Reward ( 2330 )\t -11.371795996316475\n",
      "Reward ( 2340 )\t -11.376053118944943\n",
      "Reward ( 2350 )\t -11.377718336821854\n",
      "Reward ( 2360 )\t -11.379455275781947\n",
      "Reward ( 2370 )\t -11.365422159163652\n",
      "Reward ( 2380 )\t -11.366325351937947\n",
      "Reward ( 2390 )\t -11.366536364371347\n",
      "Reward ( 2400 )\t -11.36574380958802\n",
      "Reward ( 2410 )\t -11.363100476325204\n",
      "Reward ( 2420 )\t -11.371778779893393\n",
      "Reward ( 2430 )\t -11.377714884176479\n",
      "Reward ( 2440 )\t -11.377714884176479\n",
      "Reward ( 2450 )\t -11.377396454046206\n",
      "Reward ( 2460 )\t -11.37942517666466\n",
      "Reward ( 2470 )\t -11.37942517666466\n",
      "Reward ( 2480 )\t -11.367984727683146\n",
      "Reward ( 2490 )\t -11.370203173361597\n",
      "Reward ( 2500 )\t -11.373194144258612\n",
      "Reward ( 2510 )\t -11.35004714610856\n",
      "Reward ( 2520 )\t -11.341359886889823\n",
      "Reward ( 2530 )\t -11.334039913797595\n",
      "Reward ( 2540 )\t -11.334132037002728\n",
      "Reward ( 2550 )\t -11.33288287422406\n",
      "Reward ( 2560 )\t -11.325264596165871\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00103 |      -0.02101 |      3.22e+04 |      7.99e-05 |       2.10083\n",
      "     -0.00168 |      -0.02095 |      3.21e+04 |       0.00112 |       2.09485\n",
      "     -0.00555 |      -0.02084 |      3.20e+04 |       0.00342 |       2.08391\n",
      "     -0.00631 |      -0.02073 |      3.19e+04 |       0.00648 |       2.07300\n",
      "Evaluating losses...\n",
      "     -0.00754 |      -0.02068 |      3.18e+04 |       0.00851 |       2.06818\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 101          |\n",
      "| TimestepsSoFar  | 2560         |\n",
      "| ev_tdlam_before | 0.000376     |\n",
      "| loss_ent        | 2.0681822    |\n",
      "| loss_kl         | 0.008506136  |\n",
      "| loss_pol_entpen | -0.020681823 |\n",
      "| loss_pol_surr   | -0.00753544  |\n",
      "| loss_vf_loss    | 31808.117    |\n",
      "----------------------------------\n",
      "********** Iteration 10 ************\n",
      "Reward ( 2570 )\t -11.31587458836381\n",
      "Reward ( 2580 )\t -11.316111948359167\n",
      "Reward ( 2590 )\t -11.319787949815494\n",
      "Reward ( 2600 )\t -11.313181855529129\n",
      "Reward ( 2610 )\t -11.311079098643619\n",
      "Reward ( 2620 )\t -11.289034890366914\n",
      "Reward ( 2630 )\t -11.288375359182636\n",
      "Reward ( 2640 )\t -11.276137826664074\n",
      "Reward ( 2650 )\t -11.274573019094035\n",
      "Reward ( 2660 )\t -11.264834681120183\n",
      "Reward ( 2670 )\t -11.263415195551321\n",
      "Reward ( 2680 )\t -11.255985028082229\n",
      "Reward ( 2690 )\t -11.258251934996597\n",
      "Reward ( 2700 )\t -11.268954542063266\n",
      "Reward ( 2710 )\t -11.30393010380169\n",
      "Reward ( 2720 )\t -11.305685652022214\n",
      "Reward ( 2730 )\t -11.313013946874714\n",
      "Reward ( 2740 )\t -11.311773667001562\n",
      "Reward ( 2750 )\t -11.313028620088039\n",
      "Reward ( 2760 )\t -11.31103307510101\n",
      "Reward ( 2770 )\t -11.301421417551357\n",
      "Reward ( 2780 )\t -11.309680543173764\n",
      "Reward ( 2790 )\t -11.298778359763734\n",
      "Reward ( 2800 )\t -11.272083532527017\n",
      "Reward ( 2810 )\t -11.273350685361782\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00049 |      -0.02068 |      3.17e+04 |       0.00024 |       2.06820\n",
      "     -0.01099 |      -0.02067 |      3.16e+04 |       0.00300 |       2.06724\n",
      "     -0.01520 |      -0.02065 |      3.15e+04 |       0.01101 |       2.06487\n",
      "     -0.01616 |      -0.02062 |      3.14e+04 |       0.01634 |       2.06188\n",
      "Evaluating losses...\n",
      "     -0.01855 |      -0.02060 |      3.13e+04 |       0.01638 |       2.05975\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 112          |\n",
      "| TimestepsSoFar  | 2816         |\n",
      "| ev_tdlam_before | 3.7e-05      |\n",
      "| loss_ent        | 2.059746     |\n",
      "| loss_kl         | 0.016381629  |\n",
      "| loss_pol_entpen | -0.02059746  |\n",
      "| loss_pol_surr   | -0.018550824 |\n",
      "| loss_vf_loss    | 31333.11     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 11 ************\n",
      "Reward ( 2820 )\t -11.279831404980795\n",
      "Reward ( 2830 )\t -11.280922979470915\n",
      "Reward ( 2840 )\t -11.288618356626946\n",
      "Reward ( 2850 )\t -11.28931971738638\n",
      "Reward ( 2860 )\t -11.285243888280057\n",
      "Reward ( 2870 )\t -11.300560967589055\n",
      "Reward ( 2880 )\t -11.297202762642073\n",
      "Reward ( 2890 )\t -11.290639163294232\n",
      "Reward ( 2900 )\t -11.273659732119844\n",
      "Reward ( 2910 )\t -11.26226767004135\n",
      "Reward ( 2920 )\t -11.273956399574272\n",
      "Reward ( 2930 )\t -11.286078487499655\n",
      "Reward ( 2940 )\t -11.31337990941774\n",
      "Reward ( 2950 )\t -11.294409218224477\n",
      "Reward ( 2960 )\t -11.304737525591877\n",
      "Reward ( 2970 )\t -11.304974816218438\n",
      "Reward ( 2980 )\t -11.322165335351006\n",
      "Reward ( 2990 )\t -11.30144655661107\n",
      "Reward ( 3000 )\t -11.308172511048632\n",
      "Reward ( 3010 )\t -11.303615068338125\n",
      "Reward ( 3020 )\t -11.32095229874841\n",
      "Reward ( 3030 )\t -11.336054597326743\n",
      "Reward ( 3040 )\t -11.32327307948049\n",
      "Reward ( 3050 )\t -11.306206451393809\n",
      "Reward ( 3060 )\t -11.297492541992911\n",
      "Reward ( 3070 )\t -11.295151605136306\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00171 |      -0.02059 |      3.17e+04 |       0.00036 |       2.05894\n",
      "     -0.00344 |      -0.02053 |      3.16e+04 |       0.00213 |       2.05301\n",
      "     -0.00565 |      -0.02045 |      3.15e+04 |       0.00511 |       2.04542\n",
      "     -0.00485 |      -0.02037 |      3.14e+04 |       0.00997 |       2.03711\n",
      "Evaluating losses...\n",
      "     -0.00742 |      -0.02031 |      3.13e+04 |       0.01247 |       2.03150\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 123          |\n",
      "| TimestepsSoFar  | 3072         |\n",
      "| ev_tdlam_before | 5.01e-06     |\n",
      "| loss_ent        | 2.0314953    |\n",
      "| loss_kl         | 0.012472481  |\n",
      "| loss_pol_entpen | -0.020314954 |\n",
      "| loss_pol_surr   | -0.007423565 |\n",
      "| loss_vf_loss    | 31313.387    |\n",
      "----------------------------------\n",
      "********** Iteration 12 ************\n",
      "Reward ( 3080 )\t -11.282957740490795\n",
      "Reward ( 3090 )\t -11.291710069605168\n",
      "Reward ( 3100 )\t -11.274529447366639\n",
      "Reward ( 3110 )\t -11.273080373219672\n",
      "Reward ( 3120 )\t -11.272345145782388\n",
      "Reward ( 3130 )\t -11.271201201266948\n",
      "Reward ( 3140 )\t -11.269454249227227\n",
      "Reward ( 3150 )\t -11.256546857289779\n",
      "Reward ( 3160 )\t -11.254916132123459\n",
      "Reward ( 3170 )\t -11.257377059391246\n",
      "Reward ( 3180 )\t -11.253753570691824\n",
      "Reward ( 3190 )\t -11.267419224972203\n",
      "Reward ( 3200 )\t -11.271438347854977\n",
      "Reward ( 3210 )\t -11.267905879435178\n",
      "Reward ( 3220 )\t -11.239121567819623\n",
      "Reward ( 3230 )\t -11.227178411269374\n",
      "Reward ( 3240 )\t -11.232287204289843\n",
      "Reward ( 3250 )\t -11.237212935563683\n",
      "Reward ( 3260 )\t -11.210812131106536\n",
      "Reward ( 3270 )\t -11.205920562106165\n",
      "Reward ( 3280 )\t -11.202500191977842\n",
      "Reward ( 3290 )\t -11.200546750563014\n",
      "Reward ( 3300 )\t -11.201591694695809\n",
      "Reward ( 3310 )\t -11.199650784987565\n",
      "Reward ( 3320 )\t -11.194754695220217\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00093 |      -0.02030 |      3.13e+04 |       0.00030 |       2.03023\n",
      "     -0.01432 |      -0.02023 |      3.12e+04 |       0.00402 |       2.02258\n",
      "     -0.02178 |      -0.02017 |      3.11e+04 |       0.01371 |       2.01704\n",
      "     -0.02511 |      -0.02023 |      3.10e+04 |       0.02137 |       2.02323\n",
      "Evaluating losses...\n",
      "     -0.02645 |      -0.02030 |      3.09e+04 |       0.02422 |       2.03000\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 135          |\n",
      "| TimestepsSoFar  | 3328         |\n",
      "| ev_tdlam_before | 0.000462     |\n",
      "| loss_ent        | 2.0300035    |\n",
      "| loss_kl         | 0.02422367   |\n",
      "| loss_pol_entpen | -0.020300034 |\n",
      "| loss_pol_surr   | -0.026449002 |\n",
      "| loss_vf_loss    | 30936.383    |\n",
      "----------------------------------\n",
      "********** Iteration 13 ************\n",
      "Reward ( 3330 )\t -11.162156692664926\n",
      "Reward ( 3340 )\t -11.17408270543008\n",
      "Reward ( 3350 )\t -11.173523368800055\n",
      "Reward ( 3360 )\t -11.171315882661792\n",
      "Reward ( 3370 )\t -11.166918737685737\n",
      "Reward ( 3380 )\t -11.163591370782488\n",
      "Reward ( 3390 )\t -11.17295898129604\n",
      "Reward ( 3400 )\t -11.145579946338696\n",
      "Reward ( 3410 )\t -11.1280327000337\n",
      "Reward ( 3420 )\t -11.088176687640123\n",
      "Reward ( 3430 )\t -11.05695625090794\n",
      "Reward ( 3440 )\t -11.056795314558565\n",
      "Reward ( 3450 )\t -11.062362835114863\n",
      "Reward ( 3460 )\t -11.093784451780477\n",
      "Reward ( 3470 )\t -11.105653726619991\n",
      "Reward ( 3480 )\t -11.083212057805358\n",
      "Reward ( 3490 )\t -11.087052801264894\n",
      "Reward ( 3500 )\t -11.08617384506114\n",
      "Reward ( 3510 )\t -11.087646647590253\n",
      "Reward ( 3520 )\t -11.090260547549846\n",
      "Reward ( 3530 )\t -11.085361935150557\n",
      "Reward ( 3540 )\t -11.06072461180212\n",
      "Reward ( 3550 )\t -11.052707819518776\n",
      "Reward ( 3560 )\t -11.057597109991542\n",
      "Reward ( 3570 )\t -11.040254232141962\n",
      "Reward ( 3580 )\t -11.032128133833643\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00115 |      -0.02033 |      3.04e+04 |       0.00049 |       2.03340\n",
      "     -0.01293 |      -0.02038 |      3.03e+04 |       0.00503 |       2.03791\n",
      "     -0.01897 |      -0.02034 |      3.02e+04 |       0.01691 |       2.03419\n",
      "     -0.01944 |      -0.02030 |      3.01e+04 |       0.02776 |       2.03009\n",
      "Evaluating losses...\n",
      "     -0.01939 |      -0.02026 |      3.01e+04 |       0.03120 |       2.02646\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 148          |\n",
      "| TimestepsSoFar  | 3584         |\n",
      "| ev_tdlam_before | 4.94e-05     |\n",
      "| loss_ent        | 2.0264564    |\n",
      "| loss_kl         | 0.03120405   |\n",
      "| loss_pol_entpen | -0.020264564 |\n",
      "| loss_pol_surr   | -0.019386757 |\n",
      "| loss_vf_loss    | 30090.293    |\n",
      "----------------------------------\n",
      "********** Iteration 14 ************\n",
      "Reward ( 3590 )\t -11.023298996098791\n",
      "Reward ( 3600 )\t -11.018008059041595\n",
      "Reward ( 3610 )\t -11.046507417773924\n",
      "Reward ( 3620 )\t -11.041144861499347\n",
      "Reward ( 3630 )\t -11.034865186717896\n",
      "Reward ( 3640 )\t -11.036371672071976\n",
      "Reward ( 3650 )\t -11.051997751372635\n",
      "Reward ( 3660 )\t -11.082360859074429\n",
      "Reward ( 3670 )\t -11.080769223666927\n",
      "Reward ( 3680 )\t -11.082625239962798\n",
      "Reward ( 3690 )\t -11.086142744648997\n",
      "Reward ( 3700 )\t -11.066040596182692\n",
      "Reward ( 3710 )\t -11.0614470377097\n",
      "Reward ( 3720 )\t -11.06005793575068\n",
      "Reward ( 3730 )\t -11.0625085964558\n",
      "Reward ( 3740 )\t -11.062880258904151\n",
      "Reward ( 3750 )\t -11.06119841035661\n",
      "Reward ( 3760 )\t -11.078550389110974\n",
      "Reward ( 3770 )\t -11.077168696080793\n",
      "Reward ( 3780 )\t -11.053372362802122\n",
      "Reward ( 3790 )\t -11.059039153218649\n",
      "Reward ( 3800 )\t -11.058900944124973\n",
      "Reward ( 3810 )\t -11.054597702023546\n",
      "Reward ( 3820 )\t -11.054597702023546\n",
      "Reward ( 3830 )\t -11.049217652969663\n",
      "Reward ( 3840 )\t -11.0454338200934\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00122 |      -0.02028 |      3.01e+04 |       0.00019 |       2.02754\n",
      "     -0.00826 |      -0.02030 |      3.00e+04 |       0.00154 |       2.02964\n",
      "     -0.02463 |      -0.02030 |      3.00e+04 |       0.00764 |       2.03027\n",
      "     -0.03736 |      -0.02024 |      2.99e+04 |       0.02049 |       2.02444\n",
      "Evaluating losses...\n",
      "     -0.03473 |      -0.02018 |      2.98e+04 |       0.02913 |       2.01778\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 159          |\n",
      "| TimestepsSoFar  | 3840         |\n",
      "| ev_tdlam_before | -4.29e-06    |\n",
      "| loss_ent        | 2.0177827    |\n",
      "| loss_kl         | 0.029125402  |\n",
      "| loss_pol_entpen | -0.020177826 |\n",
      "| loss_pol_surr   | -0.034726493 |\n",
      "| loss_vf_loss    | 29812.176    |\n",
      "----------------------------------\n",
      "********** Iteration 15 ************\n",
      "Reward ( 3850 )\t -11.056874078578497\n",
      "Reward ( 3860 )\t -11.03758276533911\n",
      "Reward ( 3870 )\t -11.038610348275341\n",
      "Reward ( 3880 )\t -11.032814954757624\n",
      "Reward ( 3890 )\t -11.028176501453387\n",
      "Reward ( 3900 )\t -11.021353716353625\n",
      "Reward ( 3910 )\t -11.013969600288469\n",
      "Reward ( 3920 )\t -11.020334041764599\n",
      "Reward ( 3930 )\t -11.013443672582916\n",
      "Reward ( 3940 )\t -11.017514881964537\n",
      "Reward ( 3950 )\t -11.006811591793614\n",
      "Reward ( 3960 )\t -11.01407816849042\n",
      "Reward ( 3970 )\t -10.994057213418815\n",
      "Reward ( 3980 )\t -10.987574693887986\n",
      "Reward ( 3990 )\t -10.966710753617672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward ( 4000 )\t -10.970778287200494\n",
      "Reward ( 4010 )\t -10.980491143300746\n",
      "Reward ( 4020 )\t -10.977499419925884\n",
      "Reward ( 4030 )\t -10.969538642586164\n",
      "Reward ( 4040 )\t -10.960505667195767\n",
      "Reward ( 4050 )\t -10.96691617199641\n",
      "Reward ( 4060 )\t -10.971082291006832\n",
      "Reward ( 4070 )\t -10.973072336169842\n",
      "Reward ( 4080 )\t -10.97383392810123\n",
      "Reward ( 4090 )\t -10.964759141182206\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.75e-05 |      -0.02014 |      2.97e+04 |       0.00012 |       2.01393\n",
      "     -0.00545 |      -0.02004 |      2.96e+04 |       0.00230 |       2.00420\n",
      "     -0.02284 |      -0.01988 |      2.96e+04 |       0.01140 |       1.98832\n",
      "     -0.02624 |      -0.01969 |      2.95e+04 |       0.02355 |       1.96859\n",
      "Evaluating losses...\n",
      "     -0.02448 |      -0.01964 |      2.94e+04 |       0.02687 |       1.96450\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 173          |\n",
      "| TimestepsSoFar  | 4096         |\n",
      "| ev_tdlam_before | -1.07e-06    |\n",
      "| loss_ent        | 1.9644951    |\n",
      "| loss_kl         | 0.026868828  |\n",
      "| loss_pol_entpen | -0.01964495  |\n",
      "| loss_pol_surr   | -0.024481967 |\n",
      "| loss_vf_loss    | 29411.877    |\n",
      "----------------------------------\n",
      "********** Iteration 16 ************\n",
      "Reward ( 4100 )\t -10.953760611767526\n",
      "Reward ( 4110 )\t -10.952306495321606\n",
      "Reward ( 4120 )\t -10.941735317465957\n",
      "Reward ( 4130 )\t -10.940368496368547\n",
      "Reward ( 4140 )\t -10.938819541787472\n",
      "Reward ( 4150 )\t -10.937811779186285\n",
      "Reward ( 4160 )\t -10.937972228176996\n",
      "Reward ( 4170 )\t -10.94466428758269\n",
      "Reward ( 4180 )\t -10.9374631136263\n",
      "Reward ( 4190 )\t -10.94547936528351\n",
      "Reward ( 4200 )\t -10.940241189918572\n",
      "Reward ( 4210 )\t -10.936592944387794\n",
      "Reward ( 4220 )\t -10.937895128576356\n",
      "Reward ( 4230 )\t -10.937615258470055\n",
      "Reward ( 4240 )\t -10.937589119633932\n",
      "Reward ( 4250 )\t -10.934522643344867\n",
      "Reward ( 4260 )\t -10.934903478822086\n",
      "Reward ( 4270 )\t -10.933978844642597\n",
      "Reward ( 4280 )\t -10.91724010537092\n",
      "Reward ( 4290 )\t -10.91688356584186\n",
      "Reward ( 4300 )\t -10.919006812037376\n",
      "Reward ( 4310 )\t -10.911656846728508\n",
      "Reward ( 4320 )\t -10.91577804637754\n",
      "Reward ( 4330 )\t -10.918491366244078\n",
      "Reward ( 4340 )\t -10.917816478948849\n",
      "Reward ( 4350 )\t -10.913603451687148\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00060 |      -0.01953 |      2.93e+04 |       0.00042 |       1.95308\n",
      "     -0.01571 |      -0.01915 |      2.92e+04 |       0.00455 |       1.91485\n",
      "     -0.01934 |      -0.01868 |      2.91e+04 |       0.01533 |       1.86799\n",
      "     -0.02182 |      -0.01864 |      2.91e+04 |       0.01961 |       1.86410\n",
      "Evaluating losses...\n",
      "     -0.02751 |      -0.01885 |      2.90e+04 |       0.01748 |       1.88479\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 183          |\n",
      "| TimestepsSoFar  | 4352         |\n",
      "| ev_tdlam_before | 4.05e-06     |\n",
      "| loss_ent        | 1.8847935    |\n",
      "| loss_kl         | 0.01747741   |\n",
      "| loss_pol_entpen | -0.018847935 |\n",
      "| loss_pol_surr   | -0.027505813 |\n",
      "| loss_vf_loss    | 29004.916    |\n",
      "----------------------------------\n",
      "********** Iteration 17 ************\n",
      "Reward ( 4360 )\t -10.912114771802923\n",
      "Reward ( 4370 )\t -10.909532176842212\n",
      "Reward ( 4380 )\t -10.911241262494997\n",
      "Reward ( 4390 )\t -10.907252966584231\n",
      "Reward ( 4400 )\t -10.896108907481635\n",
      "Reward ( 4410 )\t -10.898015666136367\n",
      "Reward ( 4420 )\t -10.89312694550718\n",
      "Reward ( 4430 )\t -10.894489727368782\n",
      "Reward ( 4440 )\t -10.895075841602296\n",
      "Reward ( 4450 )\t -10.879128139108884\n",
      "Reward ( 4460 )\t -10.877173756439488\n",
      "Reward ( 4470 )\t -10.87757232264172\n",
      "Reward ( 4480 )\t -10.874303344450952\n",
      "Reward ( 4490 )\t -10.870945265486926\n",
      "Reward ( 4500 )\t -10.868343341622895\n",
      "Reward ( 4510 )\t -10.858407344571782\n",
      "Reward ( 4520 )\t -10.861441194041822\n",
      "Reward ( 4530 )\t -10.85930431514088\n",
      "Reward ( 4540 )\t -10.861491060792495\n",
      "Reward ( 4550 )\t -10.860893594703255\n",
      "Reward ( 4560 )\t -10.858949720169571\n",
      "Reward ( 4570 )\t -10.85837119685003\n",
      "Reward ( 4580 )\t -10.869628403203425\n",
      "Reward ( 4590 )\t -10.868935718703426\n",
      "Reward ( 4600 )\t -10.870738455969892\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00143 |      -0.01902 |      2.90e+04 |       0.00095 |       1.90243\n",
      "     -0.00709 |      -0.01945 |      2.89e+04 |       0.00685 |       1.94458\n",
      "     -0.00819 |      -0.01972 |      2.88e+04 |       0.01714 |       1.97179\n",
      "     -0.01062 |      -0.01966 |      2.87e+04 |       0.01989 |       1.96560\n",
      "Evaluating losses...\n",
      "     -0.01555 |      -0.01949 |      2.87e+04 |       0.01630 |       1.94884\n",
      "----------------------------------\n",
      "| EpThisIter      | 0            |\n",
      "| EpisodesSoFar   | 0            |\n",
      "| TimeElapsed     | 194          |\n",
      "| TimestepsSoFar  | 4608         |\n",
      "| ev_tdlam_before | 6.97e-06     |\n",
      "| loss_ent        | 1.9488435    |\n",
      "| loss_kl         | 0.016300574  |\n",
      "| loss_pol_entpen | -0.019488435 |\n",
      "| loss_pol_surr   | -0.015547069 |\n",
      "| loss_vf_loss    | 28657.879    |\n",
      "----------------------------------\n",
      "********** Iteration 18 ************\n",
      "Reward ( 4610 )\t -10.867963964956651\n",
      "Reward ( 4620 )\t -10.86190033592197\n",
      "Reward ( 4630 )\t -10.86042502954663\n",
      "Reward ( 4640 )\t -10.85991799836217\n",
      "Reward ( 4650 )\t -10.867611708601446\n",
      "Reward ( 4660 )\t -10.86742996700794\n",
      "Reward ( 4670 )\t -10.865462768847058\n"
     ]
    }
   ],
   "source": [
    "time_steps = 200000\n",
    "model_name = \"Webots1\"\n",
    "\n",
    "model = PPO1(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=time_steps, log_interval=100)\n",
    "model.save(\"models/{}\".format(model_name))\n",
    "\n",
    "# model = PPO1.load(\"models/{}\".format('DQN_WebotFakeMini_TRPO_pj1_nReward2_200000'))\n",
    "# env = MyEnv()\n",
    "# obs = env.reset()\n",
    "\n",
    "# env.render()\n",
    "# done = False\n",
    "# max_num_steps = 100\n",
    "# time = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
