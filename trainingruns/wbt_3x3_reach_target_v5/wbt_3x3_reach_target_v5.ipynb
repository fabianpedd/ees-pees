{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../backend')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "\n",
    "import webotsgym as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webotsgym.utils import exponential_decay\n",
    "from webotsgym.env.reward import step_pen_exp\n",
    "\n",
    "class MyEval(wg.WbtReward):\n",
    "    def __init__(self, env, config):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        target_distance = self.env.get_target_distance(False)\n",
    "        if target_distance < 0.1:\n",
    "            return 10000 + 5000 * (1 - abs(self.env.state.speed))\n",
    "        else:\n",
    "            reward = 0\n",
    "            reward += -2\n",
    "            \n",
    "            if len(self.env.distances) > 0:\n",
    "                diff = self.env.distances[-1] - self.env.get_target_distance(False)\n",
    "                #print(\"Diff: \", diff)\n",
    "                \n",
    "                speed_max = 0.03\n",
    "                if abs(self.env.state.speed) > speed_max :\n",
    "                    speed_max = abs(self.env.state.speed)\n",
    "                    print(\"+++++++++++++++++ speed: \", speed_max)\n",
    "                    \n",
    "                diff_max = speed_max * 1000 / (32 * config.sim_step_every_x)\n",
    "                \n",
    "                if diff > diff_max:\n",
    "                    print(\"----------------- diff: \", diff, diff_max, \" > diff_max\")\n",
    "                # print(\"Speed: \", self.env.state.speed)\n",
    "\n",
    "                diff_rew = 0.5 * ( diff  / diff_max) / abs(self.env.state.speed / speed_max)\n",
    "                #if abs(diff_rew) > 3:\n",
    "                   # print(\"========================\")\n",
    "                   # print(\"Diff_rew:\" , diff_rew)\n",
    "                   # print(\"Diff%: \", diff  / diff_max)\n",
    "                   # print(\"Speed%: \", abs(self.env.state.speed / speed_max))\n",
    "                   # print(\"========================\")\n",
    "                    \n",
    "                if diff_rew > 3:\n",
    "                    reward += 3\n",
    "                elif diff_rew < -3:\n",
    "                    reward -= 3\n",
    "                else:\n",
    "                    reward += diff_rew\n",
    "                    \n",
    "            if self.env.state.action_denied:\n",
    "                reward += -5\n",
    "        \n",
    "            if self.env.state.touching:\n",
    "                reward += -100\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -5000:\n",
    "            print(\"reward boundary, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        if self.env.get_target_distance(False) < 0.1:\n",
    "            print(\"target reached, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        if self.env.total_reward > 25000:\n",
    "            print(\"Stop hacking!, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wg.WbtConfig()\n",
    "config.world_size = 3\n",
    "config.num_obstacles = 0\n",
    "config.sim_mode = wg.config.SimSpeedMode.FAST\n",
    "config.sim_step_every_x = 10\n",
    "config.relative_action = True\n",
    "config.direction_type = wg.config.DirectionType.STEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../backend/webotsgym/env/webotenv.py:111: UserWarning: Relative property of action class is overwritten by config.relative_action.\n",
      "  warnings.warn(\"Relative property of action class is overwritten \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n"
     ]
    }
   ],
   "source": [
    "env = wg.WbtGym(train=True, \n",
    "                evaluate_class=MyEval,\n",
    "#                 action_class = wg.WbtActDiscrete(config, dspeed=0.05, ddir=0.05),\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_name = \"3x3_reach_target_v5\"\n",
    "model = PPO1(\"MlpPolicy\", env, timesteps_per_actorbatch = 5000, tensorboard_log=\"./{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "reward boundary, reward:  -5102.15402090258\n",
      "----------------- diff:  0.09587537993207418 0.09375  > diff_max\n",
      "reward boundary, reward:  -5008.376935909232\n",
      "----------------- diff:  0.09583493180441083 0.09375  > diff_max\n",
      "reward boundary, reward:  -5098.226146618448\n",
      "target reached, reward:  -953.7493465240144\n",
      "----------------- diff:  0.09533753398455086 0.09375  > diff_max\n",
      "reward boundary, reward:  -5011.2022842597\n",
      "----------------- diff:  0.09521677110368665 0.09375  > diff_max\n",
      "reward boundary, reward:  -5033.1094885022985\n",
      "----------------- diff:  0.10926217152282536 0.09375  > diff_max\n",
      "reward boundary, reward:  -5037.957966016836\n",
      "----------------- diff:  0.09901757371178477 0.09375  > diff_max\n",
      "reward boundary, reward:  -5092.074545729583\n",
      "reward boundary, reward:  -5042.757786364394\n",
      "reward boundary, reward:  -5039.530695939796\n",
      "reward boundary, reward:  -5029.096572910092\n",
      "reward boundary, reward:  -5036.3197517205135\n",
      "reward boundary, reward:  -5045.546452101389\n",
      "target reached, reward:  -3898.760157917956\n",
      "reward boundary, reward:  -5065.95284570322\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "reward boundary, reward:  -5023.518536121002\n",
      "target reached, reward:  -46.96732047231937\n",
      "target reached, reward:  -325.96989206059277\n",
      "reward boundary, reward:  -5038.487072041036\n",
      "target reached, reward:  -767.9360954665915\n",
      "target reached, reward:  -2253.874003016648\n",
      "reward boundary, reward:  -5014.333053542954\n",
      "----------------- diff:  0.10193885366999944 0.09375  > diff_max\n",
      "----------------- diff:  0.09439202178807365 0.09375  > diff_max\n",
      "reward boundary, reward:  -5088.901760595508\n",
      "target reached, reward:  -1615.2968260390169\n",
      "----------------- diff:  0.10306429884641966 0.09375  > diff_max\n",
      "reward boundary, reward:  -5102.733004164303\n",
      "target reached, reward:  -3431.6596290227963\n",
      "reward boundary, reward:  -5000.20559248154\n",
      "target reached, reward:  -461.97319951852387\n",
      "reward boundary, reward:  -5044.862338638304\n",
      "target reached, reward:  -1070.3430326664839\n",
      "reward boundary, reward:  -5010.289043122317\n",
      "reward boundary, reward:  -5065.902949515168\n",
      "target reached, reward:  -1491.7717091742284\n",
      "target reached, reward:  -3867.9395091332963\n",
      "target reached, reward:  -1192.3645357202547\n",
      "----------------- diff:  0.096350839138585 0.09375  > diff_max\n",
      "----------------- diff:  0.0989049975292976 0.09375  > diff_max\n",
      "reward boundary, reward:  -5000.7485699720555\n",
      "target reached, reward:  -1066.4861392941636\n",
      "target reached, reward:  -1229.2030506134533\n",
      "reward boundary, reward:  -5001.857823036825\n",
      "reward boundary, reward:  -5046.864631655762\n",
      "target reached, reward:  -1923.5477460138568\n",
      "----------------- diff:  0.09985741593295056 0.09375  > diff_max\n",
      "target reached, reward:  -2998.8351428420538\n",
      "reward boundary, reward:  -5003.187875533021\n",
      "reward boundary, reward:  -5090.432288342922\n",
      "----------------- diff:  0.09429961515808372 0.09375  > diff_max\n",
      "target reached, reward:  -2153.064770495365\n",
      "reward boundary, reward:  -5009.433726971553\n",
      "target reached, reward:  -1513.128033373012\n",
      "target reached, reward:  -3996.4821929988493\n",
      "reward boundary, reward:  -5095.500482859496\n",
      "reward boundary, reward:  -5105.8375326956075\n",
      "reward boundary, reward:  -5079.92297186197\n",
      "reward boundary, reward:  -5035.196916177138\n",
      "reward boundary, reward:  -5006.653573685586\n",
      "reward boundary, reward:  -5068.809872312248\n",
      "target reached, reward:  -1231.4609253722178\n",
      "----------------- diff:  0.1015925710419846 0.09375  > diff_max\n",
      "reward boundary, reward:  -5027.9688425057375\n",
      "reward boundary, reward:  -5002.7171232280325\n",
      "----------------- diff:  0.10895180003177851 0.09375  > diff_max\n",
      "target reached, reward:  -726.2819514734268\n",
      "reward boundary, reward:  -5003.473086807761\n",
      "reward boundary, reward:  -5030.797062434932\n",
      "target reached, reward:  -4208.57373822829\n",
      "target reached, reward:  -2054.5874085571036\n",
      "reward boundary, reward:  -5035.7832681053615\n",
      "reward boundary, reward:  -5059.466690345019\n",
      "reward boundary, reward:  -5099.369816373642\n",
      "reward boundary, reward:  -5037.6302066602875\n",
      "target reached, reward:  -2947.1430640457893\n",
      "reward boundary, reward:  -5101.188196771268\n",
      "reward boundary, reward:  -5010.1185017485095\n",
      "reward boundary, reward:  -5101.086522938872\n",
      "reward boundary, reward:  -5074.230267610524\n",
      "reward boundary, reward:  -5011.19225737903\n",
      "target reached, reward:  -4479.971798799173\n",
      "reward boundary, reward:  -5000.956688652756\n",
      "reward boundary, reward:  -5002.402149749173\n",
      "reward boundary, reward:  -5070.843411373493\n",
      "reward boundary, reward:  -5059.742365296826\n",
      "reward boundary, reward:  -5107.767319432072\n",
      "target reached, reward:  -4861.658251479502\n",
      "reward boundary, reward:  -5030.488208319494\n",
      "target reached, reward:  -1167.3299520295247\n",
      "target reached, reward:  -3704.823118024131\n",
      "----------------- diff:  0.09700055512605332 0.09375  > diff_max\n",
      "reward boundary, reward:  -5000.104701242829\n",
      "target reached, reward:  -414.6539411775869\n",
      "reward boundary, reward:  -5074.106607312525\n",
      "target reached, reward:  -2486.026921179025\n",
      "----------------- diff:  0.18646063334451568 0.09375  > diff_max\n",
      "reward boundary, reward:  -5027.858508800815\n",
      "----------------- diff:  0.1144337357675198 0.09375  > diff_max\n",
      "----------------- diff:  0.10077495834270989 0.09375  > diff_max\n",
      "reward boundary, reward:  -5087.604326874586\n",
      "target reached, reward:  -3570.035848736682\n",
      "target reached, reward:  -70.82480147685996\n",
      "reward boundary, reward:  -5039.3801503831655\n",
      "reward boundary, reward:  -5006.843014177823\n",
      "target reached, reward:  -27.420710323923142\n",
      "reward boundary, reward:  -5067.501765226905\n",
      "reward boundary, reward:  -5020.8109109488705\n",
      "target reached, reward:  -543.1701811514718\n",
      "----------------- diff:  0.09416925732868547 0.09375  > diff_max\n",
      "reward boundary, reward:  -5045.890003995904\n",
      "reward boundary, reward:  -5104.817467724149\n",
      "reward boundary, reward:  -5001.113241036514\n",
      "----------------- diff:  0.0977649561191192 0.09375  > diff_max\n",
      "target reached, reward:  -443.53105725249634\n",
      "----------------- diff:  0.09408514223226933 0.09375  > diff_max\n",
      "reward boundary, reward:  -5006.623719084548\n",
      "----------------- diff:  0.09648606109468782 0.09375  > diff_max\n",
      "target reached, reward:  -3998.647264729541\n",
      "reward boundary, reward:  -5059.95414785154\n",
      "reward boundary, reward:  -5000.900736749746\n",
      "reward boundary, reward:  -5053.11141392871\n",
      "reward boundary, reward:  -5030.689097759581\n",
      "----------------- diff:  0.10597299022546136 0.09375  > diff_max\n",
      "target reached, reward:  -1970.2896607371881\n",
      "reward boundary, reward:  -5065.44399794139\n",
      "reward boundary, reward:  -5002.423183513828\n",
      "reward boundary, reward:  -5050.28628110428\n",
      "reward boundary, reward:  -5035.293063001949\n",
      "----------------- diff:  0.09857881164248739 0.09375  > diff_max\n",
      "target reached, reward:  -152.6569976027926\n",
      "reward boundary, reward:  -5061.375735422688\n",
      "reward boundary, reward:  -5031.216465146143\n",
      "reward boundary, reward:  -5088.200129609495\n",
      "reward boundary, reward:  -5001.11510195613\n",
      "target reached, reward:  -1899.2405771999543\n",
      "----------------- diff:  0.12320041995474784 0.09375  > diff_max\n",
      "target reached, reward:  -3846.7259747591806\n",
      "reward boundary, reward:  -5028.386375276154\n",
      "reward boundary, reward:  -5030.39529702786\n",
      "reward boundary, reward:  -5032.459425398916\n",
      "target reached, reward:  -974.8104874358113\n",
      "reward boundary, reward:  -5002.255836847013\n",
      "reward boundary, reward:  -5067.2830008755545\n",
      "target reached, reward:  -174.01082274895163\n",
      "reward boundary, reward:  -5089.353820835748\n",
      "target reached, reward:  -1219.9620291161627\n",
      "----------------- diff:  0.09465587872525205 0.09375  > diff_max\n",
      "----------------- diff:  0.15284561859600787 0.09375  > diff_max\n",
      "reward boundary, reward:  -5031.814469540927\n",
      "reward boundary, reward:  -5032.929725632717\n",
      "----------------- diff:  0.10507922146735876 0.09375  > diff_max\n",
      "target reached, reward:  -2494.4016382503746\n",
      "target reached, reward:  -2351.6795802003644\n",
      "reward boundary, reward:  -5061.847845927837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target reached, reward:  -3374.873377159996\n",
      "reward boundary, reward:  -5002.999305988625\n",
      "----------------- diff:  0.1091416410078283 0.09375  > diff_max\n",
      "reward boundary, reward:  -5046.626165250505\n",
      "target reached, reward:  -74.6200755394706\n",
      "reward boundary, reward:  -5050.215051321832\n",
      "reward boundary, reward:  -5001.890387323403\n",
      "reward boundary, reward:  -5001.4723881713335\n",
      "reward boundary, reward:  -5050.670979021563\n",
      "----------------- diff:  0.09648296477675089 0.09375  > diff_max\n",
      "----------------- diff:  0.09390947941528385 0.09375  > diff_max\n",
      "reward boundary, reward:  -5081.397549225317\n",
      "----------------- diff:  0.09495532171818441 0.09375  > diff_max\n",
      "----------------- diff:  0.09465535250516054 0.09375  > diff_max\n",
      "target reached, reward:  -3939.8165717409393\n",
      "reward boundary, reward:  -5009.529402398625\n",
      "reward boundary, reward:  -5016.22026304573\n",
      "reward boundary, reward:  -5091.326703639975\n",
      "reward boundary, reward:  -5002.392176542607\n",
      "----------------- diff:  0.09790443649775149 0.09375  > diff_max\n",
      "----------------- diff:  0.09392689962934142 0.09375  > diff_max\n",
      "----------------- diff:  0.10784680746381914 0.09375  > diff_max\n",
      "----------------- diff:  0.09708670614788029 0.09375  > diff_max\n",
      "----------------- diff:  0.09474020273964084 0.09375  > diff_max\n",
      "reward boundary, reward:  -5087.935670427387\n",
      "reward boundary, reward:  -5001.342674061818\n",
      "reward boundary, reward:  -5041.325957092178\n",
      "----------------- diff:  0.09859607564787973 0.09375  > diff_max\n",
      "target reached, reward:  -42.37580958518353\n",
      "reward boundary, reward:  -5050.881097239179\n",
      "reward boundary, reward:  -5036.0731949103365\n",
      "reward boundary, reward:  -5013.4095408187195\n",
      "reward boundary, reward:  -5047.413784224562\n",
      "reward boundary, reward:  -5017.106608692262\n",
      "----------------- diff:  0.10494072250214403 0.09375  > diff_max\n",
      "target reached, reward:  -4910.847063835558\n",
      "target reached, reward:  -85.63305474528892\n",
      "----------------- diff:  0.09949013980260246 0.09375  > diff_max\n",
      "reward boundary, reward:  -5014.797513758865\n",
      "reward boundary, reward:  -5094.436700318906\n",
      "target reached, reward:  -45.65349900329062\n",
      "----------------- diff:  0.0949244805299495 0.09375  > diff_max\n",
      "reward boundary, reward:  -5001.47397020919\n",
      "target reached, reward:  -367.779404307736\n",
      "reward boundary, reward:  -5055.296568591472\n",
      "target reached, reward:  -1768.4127459896713\n",
      "----------------- diff:  0.09531593496961588 0.09375  > diff_max\n",
      "target reached, reward:  -659.7965504073239\n",
      "reward boundary, reward:  -5076.108790033099\n",
      "reward boundary, reward:  -5001.306420945936\n",
      "target reached, reward:  -903.8626810381376\n",
      "----------------- diff:  0.10623194118677726 0.09375  > diff_max\n",
      "target reached, reward:  -3725.1225088788415\n",
      "reward boundary, reward:  -5019.776007082617\n",
      "target reached, reward:  -2785.4946300762617\n",
      "target reached, reward:  -182.1495128081104\n",
      "reward boundary, reward:  -5069.134232951473\n",
      "----------------- diff:  0.10645880870965185 0.09375  > diff_max\n",
      "reward boundary, reward:  -5043.977729163831\n",
      "target reached, reward:  -4971.571785087782\n",
      "reward boundary, reward:  -5095.015755753232\n",
      "target reached, reward:  -195.32434549458253\n",
      "reward boundary, reward:  -5000.905180317148\n",
      "reward boundary, reward:  -5038.665615875867\n",
      "reward boundary, reward:  -5090.443672688152\n",
      "target reached, reward:  -132.97317697263273\n",
      "reward boundary, reward:  -5008.850435209131\n",
      "----------------- diff:  0.09593798182553287 0.09375  > diff_max\n",
      "target reached, reward:  -93.46375508318657\n",
      "----------------- diff:  0.09680859748220816 0.09375  > diff_max\n",
      "target reached, reward:  -1578.328013774722\n",
      "target reached, reward:  -1873.1777975656987\n",
      "----------------- diff:  0.09953288229871701 0.09375  > diff_max\n",
      "target reached, reward:  -2280.7012332216395\n",
      "target reached, reward:  -4778.042426542603\n",
      "reward boundary, reward:  -5018.310457958932\n",
      "reward boundary, reward:  -5009.310087204833\n",
      "reward boundary, reward:  -5037.484181502394\n",
      "reward boundary, reward:  -5084.283952200043\n",
      "reward boundary, reward:  -5071.17921181923\n",
      "----------------- diff:  0.10180049259678259 0.09375  > diff_max\n",
      "reward boundary, reward:  -5070.927644908493\n",
      "target reached, reward:  -206.42178302015003\n",
      "reward boundary, reward:  -5088.395896706396\n",
      "reward boundary, reward:  -5094.3154391703065\n",
      "reward boundary, reward:  -5103.769767090666\n",
      "target reached, reward:  -84.19601825132908\n",
      "target reached, reward:  -49.976131035950026\n",
      "reward boundary, reward:  -5054.027896670872\n",
      "----------------- diff:  0.1065331743110729 0.09375  > diff_max\n",
      "target reached, reward:  -545.427979048236\n",
      "reward boundary, reward:  -5098.340363696629\n",
      "reward boundary, reward:  -5005.927757149213\n",
      "target reached, reward:  -657.9546000464941\n",
      "----------------- diff:  0.09622982008604819 0.09375  > diff_max\n",
      "target reached, reward:  -4738.457043759714\n",
      "reward boundary, reward:  -5062.521512220201\n",
      "----------------- diff:  0.10405912211045926 0.09375  > diff_max\n",
      "reward boundary, reward:  -5082.115905440494\n",
      "reward boundary, reward:  -5062.777355526095\n",
      "reward boundary, reward:  -5025.3220082137195\n",
      "----------------- diff:  0.09508251629667988 0.09375  > diff_max\n",
      "reward boundary, reward:  -5012.846735359919\n",
      "reward boundary, reward:  -5091.7297538005805\n",
      "reward boundary, reward:  -5007.1220594951255\n",
      "target reached, reward:  -2526.162632780706\n",
      "----------------- diff:  0.09735647535935499 0.09375  > diff_max\n",
      "reward boundary, reward:  -5001.879133626283\n",
      "reward boundary, reward:  -5075.560429611224\n",
      "reward boundary, reward:  -5020.666982162334\n",
      "----------------- diff:  0.09416572348218255 0.09375  > diff_max\n",
      "----------------- diff:  0.1032927373118681 0.09375  > diff_max\n",
      "target reached, reward:  -2174.6815984985556\n",
      "target reached, reward:  -157.0284377748853\n",
      "reward boundary, reward:  -5053.8381504709205\n",
      "----------------- diff:  0.11215591535056113 0.09375  > diff_max\n",
      "----------------- diff:  0.10128344337654971 0.09375  > diff_max\n",
      "reward boundary, reward:  -5064.000613125023\n",
      "----------------- diff:  0.09721103861495956 0.09375  > diff_max\n",
      "reward boundary, reward:  -5005.126566896612\n",
      "reward boundary, reward:  -5049.3437351801385\n",
      "reward boundary, reward:  -5108.604124066416\n",
      "----------------- diff:  0.09600560672769243 0.09375  > diff_max\n",
      "target reached, reward:  -3188.341452104316\n",
      "----------------- diff:  0.09585788098657028 0.09375  > diff_max\n",
      "target reached, reward:  -4100.9167491259905\n",
      "reward boundary, reward:  -5031.446352391136\n",
      "----------------- diff:  0.10785401388053506 0.09375  > diff_max\n",
      "reward boundary, reward:  -5096.9288722162855\n",
      "reward boundary, reward:  -5021.3462291089645\n",
      "target reached, reward:  -10.390623332137155\n",
      "reward boundary, reward:  -5020.542551753303\n",
      "reward boundary, reward:  -5043.913900809703\n",
      "target reached, reward:  -4918.375896756777\n",
      "----------------- diff:  0.09546888477131843 0.09375  > diff_max\n",
      "reward boundary, reward:  -5007.31628886943\n",
      "reward boundary, reward:  -5033.359441661746\n",
      "reward boundary, reward:  -5002.405366902226\n",
      "----------------- diff:  0.10072113328245014 0.09375  > diff_max\n",
      "reward boundary, reward:  -5047.56305527579\n",
      "reward boundary, reward:  -5054.553434509263\n",
      "target reached, reward:  -4321.875634289493\n",
      "reward boundary, reward:  -5071.220561330117\n",
      "reward boundary, reward:  -5069.234444110803\n",
      "target reached, reward:  -2535.7518928490717\n",
      "target reached, reward:  -4334.287309682618\n",
      "reward boundary, reward:  -5053.120470385835\n",
      "reward boundary, reward:  -5055.154906534083\n",
      "target reached, reward:  -3728.0128822741967\n",
      "reward boundary, reward:  -5017.460109649776\n",
      "reward boundary, reward:  -5051.581981037392\n",
      "reward boundary, reward:  -5015.715543495296\n",
      "target reached, reward:  -68.68018812057291\n",
      "----------------- diff:  0.09755461526729114 0.09375  > diff_max\n",
      "target reached, reward:  -788.8413617581318\n",
      "target reached, reward:  -4232.672067590762\n",
      "target reached, reward:  -145.11973242813914\n",
      "target reached, reward:  -3033.073022451168\n",
      "target reached, reward:  -4995.663612837732\n",
      "reward boundary, reward:  -5033.228203049133\n",
      "target reached, reward:  -3932.522883456672\n",
      "reward boundary, reward:  -5002.9301703638685\n",
      "reward boundary, reward:  -5094.659889377615\n",
      "reward boundary, reward:  -5068.277194287742\n",
      "target reached, reward:  -553.463819638293\n",
      "----------------- diff:  0.09534555273469048 0.09375  > diff_max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward boundary, reward:  -5074.514024860041\n",
      "reward boundary, reward:  -5051.329974612096\n",
      "reward boundary, reward:  -5096.301315035938\n",
      "----------------- diff:  0.09874687641237373 0.09375  > diff_max\n",
      "reward boundary, reward:  -5063.18840473721\n",
      "target reached, reward:  -1661.302800296745\n",
      "----------------- diff:  0.0999436771750013 0.09375  > diff_max\n",
      "reward boundary, reward:  -5026.725158573791\n",
      "target reached, reward:  -456.53438631260605\n",
      "target reached, reward:  -283.6631790865766\n",
      "reward boundary, reward:  -5005.596048438363\n",
      "----------------- diff:  0.11190233638182234 0.09375  > diff_max\n",
      "reward boundary, reward:  -5060.272149054964\n",
      "target reached, reward:  -1223.4956065437182\n",
      "reward boundary, reward:  -5032.596172838401\n",
      "----------------- diff:  0.09553099542553056 0.09375  > diff_max\n",
      "target reached, reward:  -2969.8767076805966\n",
      "----------------- diff:  0.09379539538620141 0.09375  > diff_max\n",
      "reward boundary, reward:  -5003.534593555845\n",
      "reward boundary, reward:  -5012.878742335046\n",
      "target reached, reward:  -456.8927472690802\n",
      "----------------- diff:  0.09503829293718513 0.09375  > diff_max\n",
      "reward boundary, reward:  -5003.102351943607\n",
      "----------------- diff:  0.09398560214806473 0.09375  > diff_max\n",
      "reward boundary, reward:  -5062.80599050286\n",
      "reward boundary, reward:  -5020.072354319561\n",
      "target reached, reward:  -3200.7835274208887\n",
      "target reached, reward:  -10.29602623524814\n",
      "target reached, reward:  -90.86297219386982\n",
      "target reached, reward:  -91.9857190418193\n",
      "target reached, reward:  -612.8026755413077\n",
      "reward boundary, reward:  -5048.946040253034\n",
      "reward boundary, reward:  -5029.283710659686\n",
      "reward boundary, reward:  -5068.228462303451\n",
      "reward boundary, reward:  -5080.1465486385\n",
      "reward boundary, reward:  -5024.864278392689\n",
      "reward boundary, reward:  -5106.75606089661\n",
      "target reached, reward:  -3887.0314299811826\n",
      "----------------- diff:  0.09379472692458912 0.09375  > diff_max\n",
      "reward boundary, reward:  -5087.4685222207745\n",
      "----------------- diff:  0.09474538173117786 0.09375  > diff_max\n",
      "target reached, reward:  -3509.99236874999\n",
      "target reached, reward:  -130.43932712321245\n",
      "reward boundary, reward:  -5090.082512673979\n",
      "----------------- diff:  0.09583828278913442 0.09375  > diff_max\n",
      "reward boundary, reward:  -5050.522841104784\n",
      "reward boundary, reward:  -5056.494543354247\n",
      "reward boundary, reward:  -5076.698810479343\n",
      "reward boundary, reward:  -5087.989815055989\n",
      "target reached, reward:  -834.4369253106937\n",
      "reward boundary, reward:  -5068.19777749667\n",
      "----------------- diff:  0.09999121945921596 0.09375  > diff_max\n",
      "reward boundary, reward:  -5004.639128666999\n",
      "reward boundary, reward:  -5022.207024924252\n",
      "----------------- diff:  0.09509168134541812 0.09375  > diff_max\n",
      "reward boundary, reward:  -5072.103203534442\n",
      "reward boundary, reward:  -5066.527298495163\n",
      "target reached, reward:  -2699.0059344324122\n",
      "reward boundary, reward:  -5002.8645173360665\n",
      "----------------- diff:  0.10691999446884876 0.09375  > diff_max\n",
      "----------------- diff:  0.0953058041072069 0.09375  > diff_max\n",
      "reward boundary, reward:  -5014.558736578678\n",
      "reward boundary, reward:  -5002.595613581041\n",
      "reward boundary, reward:  -5022.330850808565\n",
      "reward boundary, reward:  -5078.374442621667\n",
      "reward boundary, reward:  -5055.527732848585\n",
      "reward boundary, reward:  -5020.233338517854\n",
      "reward boundary, reward:  -5039.046381357098\n",
      "reward boundary, reward:  -5002.965817516977\n",
      "reward boundary, reward:  -5004.135699715923\n",
      "reward boundary, reward:  -5087.831943947699\n",
      "----------------- diff:  0.09421712730600851 0.09375  > diff_max\n",
      "target reached, reward:  -492.0543507263123\n",
      "target reached, reward:  -418.08328878190116\n",
      "target reached, reward:  -3232.669733580307\n",
      "reward boundary, reward:  -5058.315555556993\n",
      "----------------- diff:  0.09608919176409336 0.09375  > diff_max\n",
      "reward boundary, reward:  -5071.26382090759\n",
      "----------------- diff:  0.10278001145610288 0.09375  > diff_max\n",
      "reward boundary, reward:  -5060.306897563079\n",
      "----------------- diff:  0.10197165910753425 0.09375  > diff_max\n",
      "reward boundary, reward:  -5048.858314108292\n",
      "reward boundary, reward:  -5013.491787546548\n",
      "reward boundary, reward:  -5038.8738658160555\n",
      "target reached, reward:  -1069.9589953778743\n",
      "target reached, reward:  -1457.6718050849963\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    model.learn(total_timesteps=500000)\n",
    "    model.save(\"./{}\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
