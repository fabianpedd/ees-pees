{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../backend')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "\n",
    "import webotsgym as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webotsgym.utils import exponential_decay\n",
    "from webotsgym.env.reward import step_pen_exp\n",
    "\n",
    "class MyEval(wg.WbtReward):\n",
    "    def __init__(self, env, config):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        target_distance = self.env.get_target_distance(False)\n",
    "        if target_distance < 0.1:\n",
    "            return 5000 + 5000 * (1 - abs(self.env.state.speed))\n",
    "        else:\n",
    "            reward = 0\n",
    "            reward += -2\n",
    "            \n",
    "            if len(self.env.distances) > 0 and len(self.env.history) > 1:\n",
    "                target_diff = self.env.distances[-1] - self.env.get_target_distance(False)\n",
    "                \n",
    "                gps_diff_0 = self.env.history[-2].gps_actual[0] - self.env.state.gps_actual[0]\n",
    "                gps_diff_1 = self.env.history[-2].gps_actual[1] - self.env.state.gps_actual[1]\n",
    "                \n",
    "                #print(\"history: \", self.env.history[-1].gps_actual[0])\n",
    "                #print(\"actual:  \" , self.env.state.gps_actual[0])\n",
    "                \n",
    "                actual_diff = math.sqrt(gps_diff_0**2 + gps_diff_1**2)\n",
    "                \n",
    "                if actual_diff > 0:\n",
    "                    diff_rew = 4 * ( target_diff  / actual_diff)\n",
    "                else:\n",
    "                    diff_rew = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "                if abs(diff_rew) > 4:\n",
    "                    print(\"============= Quotient > 1 !? ==========\")\n",
    "                    print(\"Target_diff: \", target_diff)\n",
    "                    print(\"Actual_diff: \", actual_diff)\n",
    "                    print(\"diff_rew:    \", diff_rew)\n",
    "                    \n",
    "                if diff_rew < 0:\n",
    "                    diff_rew = diff_rew * 2\n",
    "                \n",
    "                #print(\"Target_diff: \", target_diff)\n",
    "                #print(\"Actual_diff: \", actual_diff)\n",
    "                #print(\"diff_rew:    \", diff_rew)  \n",
    "                    \n",
    "  \n",
    "                reward += diff_rew\n",
    "                    \n",
    "            if self.env.state.action_denied:\n",
    "                reward += -5\n",
    "        \n",
    "            if self.env.state.touching:\n",
    "                reward += -100\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -5000:\n",
    "            print(\"reward boundary, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        if self.env.get_target_distance(False) < 0.1:\n",
    "            print(\"target reached, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        if self.env.total_reward > 25000:\n",
    "            print(\"Stop hacking!, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wg.WbtConfig()\n",
    "config.world_size = 3\n",
    "config.num_obstacles = 0\n",
    "config.sim_mode = wg.config.SimSpeedMode.FAST\n",
    "config.sim_step_every_x = 10\n",
    "config.relative_action = True\n",
    "config.direction_type = wg.config.DirectionType.STEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../backend/webotsgym/env/webotenv.py:111: UserWarning: Relative property of action class is overwritten by config.relative_action.\n",
      "  warnings.warn(\"Relative property of action class is overwritten \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n"
     ]
    }
   ],
   "source": [
    "env = wg.WbtGym(train=True, \n",
    "                evaluate_class=MyEval,\n",
    "#                 action_class = wg.WbtActDiscrete(config, dspeed=0.05, ddir=0.05),\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_name = \"3x3_reach_target_v6\"\n",
    "model = PPO1(\"MlpPolicy\", env, timesteps_per_actorbatch = 5000, tensorboard_log=\"./{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "reward boundary, reward:  -5075.138390909345\n",
      "reward boundary, reward:  -5013.254815767437\n",
      "reward boundary, reward:  -5033.5091386599715\n",
      "reward boundary, reward:  -5087.786258716982\n",
      "reward boundary, reward:  -5024.045613093157\n",
      "reward boundary, reward:  -5008.509655753781\n",
      "reward boundary, reward:  -5054.38617389161\n",
      "reward boundary, reward:  -5080.072414041448\n",
      "reward boundary, reward:  -5100.005598459242\n",
      "reward boundary, reward:  -5067.950279860842\n",
      "reward boundary, reward:  -5011.904305615229\n",
      "target reached, reward:  -1769.3919054887645\n",
      "target reached, reward:  -1506.5026336757157\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "reward boundary, reward:  -5001.076360459701\n",
      "reward boundary, reward:  -5003.74068047095\n",
      "target reached, reward:  -2775.7471875119295\n",
      "reward boundary, reward:  -5075.205207944889\n",
      "reward boundary, reward:  -5104.994773197897\n",
      "target reached, reward:  -4490.768425756203\n",
      "reward boundary, reward:  -5004.264622656174\n",
      "reward boundary, reward:  -5019.834311626317\n",
      "target reached, reward:  -1318.1083992078989\n",
      "reward boundary, reward:  -5005.063776536096\n",
      "target reached, reward:  -4851.47342119092\n",
      "target reached, reward:  -3514.6029183499477\n",
      "reward boundary, reward:  -5065.95391190359\n",
      "reward boundary, reward:  -5076.226525746286\n",
      "reward boundary, reward:  -5039.264587977526\n",
      "reward boundary, reward:  -5013.325337243372\n",
      "reward boundary, reward:  -5004.077963178702\n",
      "reward boundary, reward:  -5003.726969560871\n",
      "reward boundary, reward:  -5054.9486116828675\n",
      "reward boundary, reward:  -5067.1216810619135\n",
      "target reached, reward:  -2559.456896712912\n",
      "target reached, reward:  -6.271785665735692\n",
      "target reached, reward:  -1906.5438873269086\n",
      "reward boundary, reward:  -5012.909702734358\n",
      "reward boundary, reward:  -5070.461920434309\n",
      "reward boundary, reward:  -5058.858582035821\n",
      "reward boundary, reward:  -5002.324513234411\n",
      "target reached, reward:  -515.5737666881814\n",
      "reward boundary, reward:  -5003.314242458756\n",
      "target reached, reward:  -188.0883482294379\n",
      "target reached, reward:  -165.07811055490535\n",
      "target reached, reward:  -2438.9435112606443\n",
      "target reached, reward:  -205.3629904957409\n",
      "reward boundary, reward:  -5101.0981357402225\n",
      "reward boundary, reward:  -5030.649071709213\n",
      "target reached, reward:  -685.5773975721542\n",
      "reward boundary, reward:  -5096.578264395746\n",
      "target reached, reward:  -1231.3260363461623\n",
      "target reached, reward:  -3591.4078344092404\n",
      "target reached, reward:  -109.62092951238415\n",
      "reward boundary, reward:  -5002.038635906089\n",
      "reward boundary, reward:  -5092.038004916612\n",
      "reward boundary, reward:  -5036.498180066119\n",
      "reward boundary, reward:  -5064.702437676322\n",
      "reward boundary, reward:  -5105.396951763153\n",
      "target reached, reward:  -539.7413705714256\n",
      "reward boundary, reward:  -5090.0378773140455\n",
      "reward boundary, reward:  -5096.275028466223\n",
      "reward boundary, reward:  -5049.953427536288\n",
      "reward boundary, reward:  -5021.19225925563\n",
      "reward boundary, reward:  -5055.649756938557\n",
      "reward boundary, reward:  -5083.248160474642\n",
      "reward boundary, reward:  -5063.735465217856\n",
      "reward boundary, reward:  -5006.316876476216\n",
      "reward boundary, reward:  -5006.7246175755\n",
      "reward boundary, reward:  -5003.380932552726\n",
      "reward boundary, reward:  -5083.045337471179\n",
      "reward boundary, reward:  -5003.914135613166\n",
      "target reached, reward:  11.788247269480731\n",
      "reward boundary, reward:  -5066.787403244662\n",
      "target reached, reward:  -1632.8573333774407\n",
      "reward boundary, reward:  -5009.282786045721\n",
      "target reached, reward:  -190.46122205145934\n",
      "target reached, reward:  -4054.1731904120998\n",
      "target reached, reward:  -564.6555481553752\n",
      "reward boundary, reward:  -5035.852715171033\n",
      "reward boundary, reward:  -5030.446847683594\n",
      "reward boundary, reward:  -5004.279164554818\n",
      "reward boundary, reward:  -5086.622905039487\n",
      "reward boundary, reward:  -5020.327695727085\n",
      "reward boundary, reward:  -5020.9761669170475\n",
      "reward boundary, reward:  -5049.774773633012\n",
      "reward boundary, reward:  -5022.784719086536\n",
      "target reached, reward:  -234.7301372552886\n",
      "reward boundary, reward:  -5034.434933131661\n",
      "reward boundary, reward:  -5057.135884360981\n",
      "reward boundary, reward:  -5070.86186942769\n",
      "target reached, reward:  -618.3360221533952\n",
      "reward boundary, reward:  -5048.791508429857\n",
      "target reached, reward:  6.754785053218052\n",
      "target reached, reward:  12.775219302697685\n",
      "target reached, reward:  -1686.381717024153\n",
      "reward boundary, reward:  -5002.831239233127\n",
      "reward boundary, reward:  -5084.081527805253\n",
      "reward boundary, reward:  -5001.20049963696\n",
      "reward boundary, reward:  -5006.7278684728135\n",
      "reward boundary, reward:  -5000.720306478692\n",
      "target reached, reward:  -14.916045079970026\n",
      "reward boundary, reward:  -5030.044161178338\n",
      "reward boundary, reward:  -5003.687836510387\n",
      "target reached, reward:  -106.77988231539278\n",
      "target reached, reward:  -4673.36108683816\n",
      "reward boundary, reward:  -5001.612080042107\n",
      "target reached, reward:  -2074.047051431001\n",
      "reward boundary, reward:  -5106.876012719396\n",
      "target reached, reward:  -3844.9383427712337\n",
      "target reached, reward:  -2267.710261333915\n",
      "reward boundary, reward:  -5002.764172936824\n",
      "target reached, reward:  -653.6318820263594\n",
      "reward boundary, reward:  -5020.001807796876\n",
      "target reached, reward:  -1492.2570594474396\n",
      "target reached, reward:  -1853.1450252301909\n",
      "reward boundary, reward:  -5090.864745472054\n",
      "target reached, reward:  -958.2457787485151\n",
      "target reached, reward:  -34.27200672151343\n",
      "reward boundary, reward:  -5002.2019546970205\n",
      "reward boundary, reward:  -5044.819385993842\n",
      "reward boundary, reward:  -5083.0479584031\n",
      "reward boundary, reward:  -5029.559768868928\n",
      "target reached, reward:  -1216.4975937806632\n",
      "target reached, reward:  -2756.064285933104\n",
      "target reached, reward:  -2170.643773675996\n",
      "reward boundary, reward:  -5002.27800133654\n",
      "reward boundary, reward:  -5092.554049834965\n",
      "reward boundary, reward:  -5024.0925093765645\n",
      "reward boundary, reward:  -5024.522971563349\n",
      "reward boundary, reward:  -5083.703987708638\n",
      "reward boundary, reward:  -5079.602628022291\n",
      "target reached, reward:  -5.4714924055873215\n",
      "reward boundary, reward:  -5054.302423409218\n",
      "reward boundary, reward:  -5010.192862288717\n",
      "reward boundary, reward:  -5003.1001007038685\n",
      "target reached, reward:  -2248.7533611764607\n",
      "reward boundary, reward:  -5004.605572512755\n",
      "reward boundary, reward:  -5084.677840170058\n",
      "reward boundary, reward:  -5073.58225125875\n",
      "reward boundary, reward:  -5035.625746272947\n",
      "target reached, reward:  -4904.589947134524\n",
      "reward boundary, reward:  -5034.71051347763\n",
      "reward boundary, reward:  -5040.4833764980685\n",
      "reward boundary, reward:  -5005.7144167012675\n",
      "reward boundary, reward:  -5080.353328876475\n",
      "target reached, reward:  -1018.1549803355658\n",
      "reward boundary, reward:  -5046.666038620663\n",
      "reward boundary, reward:  -5007.183498657158\n",
      "reward boundary, reward:  -5101.064967427825\n",
      "target reached, reward:  -4098.778629612667\n",
      "reward boundary, reward:  -5023.645561916104\n",
      "reward boundary, reward:  -5011.971761360712\n",
      "reward boundary, reward:  -5002.783381816183\n",
      "reward boundary, reward:  -5093.825666629181\n",
      "target reached, reward:  -22.750247540146184\n",
      "reward boundary, reward:  -5000.8466961552\n",
      "reward boundary, reward:  -5027.131452029736\n",
      "target reached, reward:  -4237.83581290355\n",
      "target reached, reward:  -350.3757153469849\n",
      "reward boundary, reward:  -5026.136180978354\n",
      "reward boundary, reward:  -5003.552129047722\n",
      "target reached, reward:  -2997.523580240941\n",
      "target reached, reward:  -1060.038897951637\n",
      "target reached, reward:  -2448.4917121705776\n",
      "reward boundary, reward:  -5018.472591664871\n",
      "reward boundary, reward:  -5102.435691684461\n",
      "target reached, reward:  -172.37689911954467\n",
      "reward boundary, reward:  -5002.4319666273705\n",
      "reward boundary, reward:  -5013.048820867071\n",
      "reward boundary, reward:  -5013.141709711387\n",
      "target reached, reward:  -1165.6453620311229\n",
      "target reached, reward:  -1567.3751373592054\n",
      "target reached, reward:  -50.989333542818194\n",
      "reward boundary, reward:  -5004.0907898430105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target reached, reward:  -1801.6668112116356\n",
      "reward boundary, reward:  -5005.370643129012\n",
      "target reached, reward:  -26.13245404180086\n",
      "target reached, reward:  10.723849348179156\n",
      "reward boundary, reward:  -5005.429028366687\n",
      "reward boundary, reward:  -5037.600346670333\n",
      "reward boundary, reward:  -5096.891715493432\n",
      "reward boundary, reward:  -5031.034926587288\n",
      "target reached, reward:  -1406.3511001691481\n",
      "reward boundary, reward:  -5059.31741492684\n",
      "target reached, reward:  -1807.6137520876023\n",
      "reward boundary, reward:  -5020.189858048437\n",
      "target reached, reward:  -2155.6087686127294\n",
      "target reached, reward:  -2306.0624475616446\n",
      "target reached, reward:  -579.3767082489529\n",
      "reward boundary, reward:  -5065.743816013294\n",
      "reward boundary, reward:  -5114.091098103273\n",
      "target reached, reward:  -3516.1488047165\n",
      "reward boundary, reward:  -5078.493407652511\n",
      "target reached, reward:  -1663.1247570877745\n",
      "reward boundary, reward:  -5038.911222730515\n",
      "reward boundary, reward:  -5097.523006580834\n",
      "reward boundary, reward:  -5006.842348735154\n",
      "reward boundary, reward:  -5061.333038976908\n",
      "reward boundary, reward:  -5004.950558289319\n",
      "target reached, reward:  -3396.612922819518\n",
      "target reached, reward:  -466.24876317694105\n",
      "target reached, reward:  -81.21046103389156\n",
      "reward boundary, reward:  -5000.991747097705\n",
      "reward boundary, reward:  -5021.296236306096\n",
      "target reached, reward:  -3421.3589557837813\n",
      "reward boundary, reward:  -5083.740969479905\n",
      "target reached, reward:  -54.62534634475346\n",
      "target reached, reward:  -427.4024492673375\n",
      "reward boundary, reward:  -5074.386730034808\n",
      "target reached, reward:  -3057.2351134854293\n",
      "target reached, reward:  -1075.0471855513497\n",
      "reward boundary, reward:  -5006.149880474232\n",
      "reward boundary, reward:  -5032.304881029579\n",
      "reward boundary, reward:  -5009.969959571982\n",
      "reward boundary, reward:  -5089.680209511364\n",
      "target reached, reward:  -640.8345356694477\n",
      "reward boundary, reward:  -5041.191363427031\n",
      "reward boundary, reward:  -5040.308745686547\n",
      "reward boundary, reward:  -5029.193187749082\n",
      "reward boundary, reward:  -5076.292126068678\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    model.learn(total_timesteps=500000)\n",
    "    model.save(\"./{}\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
