{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../backend')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "\n",
    "import webotsgym as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webotsgym.utils import exponential_decay\n",
    "from webotsgym.env.reward import step_pen_exp\n",
    "\n",
    "class MyEval(wg.WbtReward):\n",
    "    def __init__(self, env, config):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        target_distance = self.env.get_target_distance(False)\n",
    "        if target_distance < 0.1:\n",
    "            print(\"Calc_reward target bonus: \", 500 + 500 * (1 - abs(self.env.state.speed)))\n",
    "            return 500 + 500 * (1 - abs(self.env.state.speed))\n",
    "        else:\n",
    "            reward = 0\n",
    "            reward += -1\n",
    "            env.step_pen -= 1\n",
    "            \n",
    "            if len(self.env.distances) > 0 and len(self.env.history) > 0:\n",
    "                gps_start_0 = self.env.history[0].gps_actual[0] - self.env.gps_target[0]\n",
    "                gps_start_1 = self.env.history[0].gps_actual[1] - self.env.gps_target[1]\n",
    "                \n",
    "                start_dist = math.sqrt(gps_start_0**2 + gps_start_1**2)\n",
    "                \n",
    "                \n",
    "                \n",
    "                target_diff = self.env.distances[-1] - self.env.get_target_distance(False)\n",
    "                \n",
    "                gps_diff_0 = self.env.history[-1].gps_actual[0] - self.env.state.gps_actual[0]\n",
    "                gps_diff_1 = self.env.history[-1].gps_actual[1] - self.env.state.gps_actual[1]\n",
    "                \n",
    "                #print(\"history: \", self.env.history[-1].gps_actual[0])\n",
    "                #print(\"actual:  \" , self.env.state.gps_actual[0])\n",
    "                \n",
    "                actual_diff = math.sqrt(gps_diff_0**2 + gps_diff_1**2)\n",
    "                \n",
    "                if actual_diff > 0:\n",
    "                    diff_rew = abs(target_diff / start_dist * 500) * ( target_diff  / actual_diff)\n",
    "                else:\n",
    "                    diff_rew = 0\n",
    "                    \n",
    "                                \n",
    "                env.dist_rew.append(diff_rew)\n",
    "                if sum(env.dist_rew) > 500:\n",
    "                    print(\"Dist_rew too big: \", sum(env.dist_rew))\n",
    "                    \n",
    "                env.t_diff.append(target_diff)\n",
    "                if sum(env.t_diff) > start_dist:\n",
    "                    print(\"Sum of t_diff to big: \", sum(env.t_diff))\n",
    "                    print(\"start_dist: \",start_dist)\n",
    "               \n",
    "                    \n",
    "                #if diff_rew < 0:\n",
    "                #    diff_rew = diff_rew * 2\n",
    "                \n",
    "                #print(\"Target_diff: \", target_diff)\n",
    "                #print(\"Actual_diff: \", actual_diff)\n",
    "                #print(\"start_dist: \", start_dist)\n",
    "                #print(\"diff_rew:   \", diff_rew)  \n",
    "                    \n",
    "  \n",
    "                reward += diff_rew\n",
    "                    \n",
    "            if self.env.state.action_denied:\n",
    "                reward += -1\n",
    "        \n",
    "            if self.env.state.touching:\n",
    "                reward += -20\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -500:\n",
    "            print(\"reward boundary, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        if self.env.step_pen < -300:\n",
    "            print(\"step boundary, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        if self.env.get_target_distance(False) < 0.1:\n",
    "            print(\"target reached, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        if self.env.total_reward > 2000:\n",
    "            print(\"Stop hacking!, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wg.WbtConfig()\n",
    "config.world_size = 3\n",
    "config.num_obstacles = 0\n",
    "config.sim_mode = wg.config.SimSpeedMode.FAST\n",
    "config.sim_step_every_x = 5\n",
    "config.relative_action = True\n",
    "config.direction_type = wg.config.DirectionType.STEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../backend/webotsgym/env/webotenv.py:123: UserWarning: Relative property of action class is overwritten by config.relative_action. This might interfere with bounds argument for WbtActContinuous.\n",
      "  warnings.warn(\"Relative property of action class is overwritten \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n"
     ]
    }
   ],
   "source": [
    "env = wg.WbtGym(train=True, \n",
    "                evaluate_class=MyEval,\n",
    "               # action_class = wg.WbtActContinuous(config=config, relative = True),\n",
    "                action_class = wg.WbtActDiscrete(config = config, dspeed = 0.1, ddir = 0.1, speeds = 3, dirs = 3),\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dist_rew = []\n",
    "env.t_diff = []\n",
    "env.step_pen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_name = \"3x3_reach_target_v7_b\"\n",
    "model = PPO1(\"MlpPolicy\", env, timesteps_per_actorbatch = 2000, tensorboard_log=\"./{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "step boundary, reward:  -294.9376384780239\n",
      "              dist_rew :  21.06236152197613\n",
      "step boundary, reward:  -167.04474616617927\n",
      "              dist_rew :  137.9552538338209\n",
      "step boundary, reward:  -258.72995785093485\n",
      "              dist_rew :  51.27004214906514\n",
      "reward boundary, reward:  -509.7002020449443\n",
      "              dist_rew :  -450.7002020449443\n",
      "reward boundary, reward:  -504.9392135085392\n",
      "              dist_rew :  -262.93921350853884\n",
      "step boundary, reward:  -248.21353013962155\n",
      "              dist_rew :  72.78646986037856\n",
      "step boundary, reward:  -370.6668419549226\n",
      "              dist_rew :  -44.6668419549228\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "step boundary, reward:  -437.42847710893864\n",
      "              dist_rew :  61.57152289106154\n",
      "reward boundary, reward:  -503.07682643683074\n",
      "              dist_rew :  -228.0768264368309\n",
      "step boundary, reward:  -437.86091880334493\n",
      "              dist_rew :  -112.86091880334496\n",
      "step boundary, reward:  -102.24693578968736\n",
      "              dist_rew :  250.75306421031274\n",
      "step boundary, reward:  -326.9151777620882\n",
      "              dist_rew :  -6.915177762088176\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    model.learn(total_timesteps=500000)\n",
    "    model.save(\"./model_{}\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
