{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../backend')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "\n",
    "import webotsgym as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webotsgym.utils import exponential_decay\n",
    "from webotsgym.env.reward import step_pen_exp\n",
    "\n",
    "class MyEval(wg.WbtReward):\n",
    "    def __init__(self, env, config):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        target_distance = self.env.get_target_distance(False)\n",
    "        if target_distance < 0.1:\n",
    "            print(\"Calc_reward target bonus: \", 5000 + 5000 * (1 - abs(self.env.state.speed)/0.03))\n",
    "            return 5000 + 5000 * (1 - abs(self.env.state.speed)/0.03)\n",
    "        else:\n",
    "            reward = 0\n",
    "            reward += -10\n",
    "            env.step_pen -= 10\n",
    "            \n",
    "            if len(self.env.distances) > 0 and len(self.env.history) > 0:\n",
    "                gps_start_0 = self.env.history[0].gps_actual[0] - self.env.gps_target[0]\n",
    "                gps_start_1 = self.env.history[0].gps_actual[1] - self.env.gps_target[1]\n",
    "                \n",
    "                start_dist = math.sqrt(gps_start_0**2 + gps_start_1**2)\n",
    "                \n",
    "                \n",
    "                \n",
    "                target_diff = self.env.distances[-1] - self.env.get_target_distance(False)\n",
    "                \n",
    "                gps_diff_0 = self.env.history[-1].gps_actual[0] - self.env.state.gps_actual[0]\n",
    "                gps_diff_1 = self.env.history[-1].gps_actual[1] - self.env.state.gps_actual[1]\n",
    "                \n",
    "                #print(\"history: \", self.env.history[-1].gps_actual[0])\n",
    "                #print(\"actual:  \" , self.env.state.gps_actual[0])\n",
    "                \n",
    "                actual_diff = math.sqrt(gps_diff_0**2 + gps_diff_1**2)\n",
    "                \n",
    "                if actual_diff > 0:\n",
    "                    diff_rew = abs(target_diff / start_dist * 5000) * ( target_diff  / actual_diff)\n",
    "                else:\n",
    "                    diff_rew = 0\n",
    "                    \n",
    "                                \n",
    "                env.dist_rew.append(diff_rew)\n",
    "                if sum(env.dist_rew) > 5000:\n",
    "                    print(\"Dist_rew too big: \", sum(env.dist_rew))\n",
    "                    \n",
    "                env.t_diff.append(target_diff)\n",
    "                if sum(env.t_diff) > start_dist:\n",
    "                    print(\"Sum of t_diff to big: \", sum(env.t_diff))\n",
    "                    print(\"start_dist: \",start_dist)\n",
    "               \n",
    "                    \n",
    "                #if diff_rew < 0:\n",
    "                #    diff_rew = diff_rew * 2\n",
    "                \n",
    "                #print(\"Target_diff: \", target_diff)\n",
    "                #print(\"Actual_diff: \", actual_diff)\n",
    "                #print(\"start_dist: \", start_dist)\n",
    "                #print(\"diff_rew:   \", diff_rew)  \n",
    "                    \n",
    "  \n",
    "                reward += diff_rew\n",
    "                    \n",
    "            if self.env.state.action_denied:\n",
    "                reward += -10\n",
    "        \n",
    "            if self.env.state.touching:\n",
    "                reward += -200\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -5000:\n",
    "            print(\"reward boundary, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        if self.env.step_pen < -2000:\n",
    "            print(\"reward boundary, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        if self.env.get_target_distance(False) < 0.1:\n",
    "            print(\"target reached, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        if self.env.total_reward > 25000:\n",
    "            print(\"Stop hacking!, reward: \", self.env.total_reward)\n",
    "            print(\"              dist_rew : \", sum(env.dist_rew))\n",
    "            env.dist_rew.clear()\n",
    "            env.t_diff.clear()\n",
    "            env.step_pen = 0\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wg.WbtConfig()\n",
    "config.world_size = 3\n",
    "config.num_obstacles = 0\n",
    "config.sim_mode = wg.config.SimSpeedMode.FAST\n",
    "config.sim_step_every_x = 10\n",
    "config.relative_action = True\n",
    "config.direction_type = wg.config.DirectionType.STEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../backend/webotsgym/env/webotenv.py:123: UserWarning: Relative property of action class is overwritten by config.relative_action. This might interfere with bounds argument for WbtActContinuous.\n",
      "  warnings.warn(\"Relative property of action class is overwritten \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n"
     ]
    }
   ],
   "source": [
    "env = wg.WbtGym(train=True, \n",
    "                evaluate_class=MyEval,\n",
    "                action_class = wg.WbtActContinuous(config=config, relative = True),\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dist_rew = []\n",
    "env.t_diff = []\n",
    "env.step_pen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_name = \"3x3_reach_target_v7_b\"\n",
    "model = PPO1(\"MlpPolicy\", env, timesteps_per_actorbatch = 5000, tensorboard_log=\"./{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "reward boundary, reward:  -5303.018350164373\n",
      "              dist_rew :  -1573.0183501643728\n",
      "reward boundary, reward:  -1131.8411488231866\n",
      "              dist_rew :  1878.1588511768161\n",
      "reward boundary, reward:  -5202.758038568446\n",
      "              dist_rew :  -52.75803856844497\n",
      "reward boundary, reward:  -5098.165453368743\n",
      "              dist_rew :  1011.8345466312575\n",
      "reward boundary, reward:  -5243.583650487784\n",
      "              dist_rew :  -983.5836504877822\n",
      "reward boundary, reward:  -5084.801317162799\n",
      "              dist_rew :  -2944.801317162801\n",
      "reward boundary, reward:  -2782.4448873668885\n",
      "              dist_rew :  227.55511263311178\n",
      "reward boundary, reward:  -5093.467736244185\n",
      "              dist_rew :  -1473.467736244188\n",
      "reward boundary, reward:  -5021.539483073175\n",
      "              dist_rew :  -321.53948307317376\n",
      "reward boundary, reward:  -2177.2414462663814\n",
      "              dist_rew :  832.758553733619\n",
      "reward boundary, reward:  -5194.276347533745\n",
      "              dist_rew :  -604.2763475337466\n",
      "reward boundary, reward:  -5050.806763808361\n",
      "              dist_rew :  3149.1932361916397\n",
      "reward boundary, reward:  -5047.601684507135\n",
      "              dist_rew :  2612.3983154928646\n",
      "reward boundary, reward:  -5089.233735285347\n",
      "              dist_rew :  -639.233735285348\n",
      "reward boundary, reward:  -5114.483765126722\n",
      "              dist_rew :  -404.4837651267212\n",
      "reward boundary, reward:  -2390.886325944679\n",
      "              dist_rew :  619.1136740553227\n",
      "reward boundary, reward:  -2653.586918467673\n",
      "              dist_rew :  356.4130815323261\n",
      "reward boundary, reward:  -2626.3132686572185\n",
      "              dist_rew :  383.6867313427812\n",
      "reward boundary, reward:  -5148.209213058711\n",
      "              dist_rew :  -2838.2092130587107\n",
      "reward boundary, reward:  -5198.188846813598\n",
      "              dist_rew :  -548.1888468135976\n",
      "reward boundary, reward:  -5002.797682293702\n",
      "              dist_rew :  -2432.7976822936994\n",
      "reward boundary, reward:  -2456.8548554897507\n",
      "              dist_rew :  553.145144510248\n",
      "reward boundary, reward:  -5077.52373907747\n",
      "              dist_rew :  212.47626092252918\n",
      "reward boundary, reward:  -5111.731399515801\n",
      "              dist_rew :  -151.7313995158012\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "reward boundary, reward:  -2217.8109098050995\n",
      "              dist_rew :  792.1890901949002\n",
      "reward boundary, reward:  -5154.12450441467\n",
      "              dist_rew :  -464.1245044146708\n",
      "reward boundary, reward:  -2537.3678252936907\n",
      "              dist_rew :  472.63217470630906\n",
      "reward boundary, reward:  -2984.759132838882\n",
      "              dist_rew :  25.24086716111583\n",
      "reward boundary, reward:  -3243.7716016830514\n",
      "              dist_rew :  -183.7716016830501\n",
      "reward boundary, reward:  -3326.128161626844\n",
      "              dist_rew :  -316.12816162684834\n",
      "reward boundary, reward:  -3303.7197383607418\n",
      "              dist_rew :  -293.7197383607405\n",
      "reward boundary, reward:  -4019.0334195944115\n",
      "              dist_rew :  -879.0334195944084\n",
      "reward boundary, reward:  -3384.4231532712834\n",
      "              dist_rew :  -374.4231532712821\n",
      "reward boundary, reward:  -2955.94931752701\n",
      "              dist_rew :  54.050682472990495\n",
      "reward boundary, reward:  -2174.456125157428\n",
      "              dist_rew :  835.543874842572\n",
      "reward boundary, reward:  -3195.3750815228645\n",
      "              dist_rew :  -125.37508152286439\n",
      "reward boundary, reward:  -5220.339696861371\n",
      "              dist_rew :  -1150.3396968613704\n",
      "reward boundary, reward:  -1254.5836983929169\n",
      "              dist_rew :  1755.416301607082\n",
      "reward boundary, reward:  -5202.647806383916\n",
      "              dist_rew :  -642.6478063839165\n",
      "reward boundary, reward:  -2904.8108130331684\n",
      "              dist_rew :  105.1891869668334\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    model.learn(total_timesteps=500000)\n",
    "    model.save(\"./model_{}\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
