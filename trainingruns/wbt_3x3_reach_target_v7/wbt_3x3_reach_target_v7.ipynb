{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../backend')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import stable_baselines\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, DDPG, SAC, PPO1, PPO2, TD3, TRPO\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "\n",
    "import webotsgym as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webotsgym.utils import exponential_decay\n",
    "from webotsgym.env.reward import step_pen_exp\n",
    "\n",
    "class MyEval(wg.WbtReward):\n",
    "    def __init__(self, env, config):\n",
    "        super(MyEval, self).__init__(env, config)\n",
    "\n",
    "    def calc_reward(self):\n",
    "        target_distance = self.env.get_target_distance(False)\n",
    "        if target_distance < 0.1:\n",
    "            print(\"Calc_reward target bonus: \", 5000 + 5000 * (1 - abs(self.env.state.speed)/0.03))\n",
    "            return 5000 + 5000 * (1 - abs(self.env.state.speed)/0.03)\n",
    "        else:\n",
    "            reward = 0\n",
    "            reward += -2\n",
    "            \n",
    "            if len(self.env.distances) > 0 and len(self.env.history) > 1:\n",
    "                target_diff = self.env.distances[-1] - self.env.get_target_distance(False)\n",
    "                \n",
    "                gps_diff_0 = self.env.history[-1].gps_actual[0] - self.env.state.gps_actual[0]\n",
    "                gps_diff_1 = self.env.history[-1].gps_actual[1] - self.env.state.gps_actual[1]\n",
    "                \n",
    "                #print(\"history: \", self.env.history[-1].gps_actual[0])\n",
    "                #print(\"actual:  \" , self.env.state.gps_actual[0])\n",
    "                \n",
    "                actual_diff = math.sqrt(gps_diff_0**2 + gps_diff_1**2)\n",
    "                \n",
    "                if actual_diff > 0:\n",
    "                    diff_rew = 4 * ( target_diff  / actual_diff)\n",
    "                else:\n",
    "                    diff_rew = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "                if abs(diff_rew) > 4:\n",
    "                    print(\"============= Quotient > 1 !? ==========\")\n",
    "                    print(\"Target_diff: \", target_diff)\n",
    "                    print(\"Actual_diff: \", actual_diff)\n",
    "                    print(\"diff_rew:    \", diff_rew)\n",
    "                    \n",
    "                if diff_rew < 0:\n",
    "                    diff_rew = diff_rew * 2\n",
    "                \n",
    "                #print(\"Target_diff: \", target_diff)\n",
    "                #print(\"Actual_diff: \", actual_diff)\n",
    "                #print(\"diff_rew:    \", diff_rew)  \n",
    "                    \n",
    "  \n",
    "                reward += diff_rew\n",
    "                    \n",
    "            if self.env.state.action_denied:\n",
    "                reward += -5\n",
    "        \n",
    "            if self.env.state.touching:\n",
    "                reward += -100\n",
    "        return reward\n",
    "\n",
    "    def check_done(self):\n",
    "        if self.env.total_reward < -5000:\n",
    "            print(\"reward boundary, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        if self.env.get_target_distance(False) < 0.1:\n",
    "            print(\"target reached, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        if self.env.total_reward > 25000:\n",
    "            print(\"Stop hacking!, reward: \", self.env.total_reward)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wg.WbtConfig()\n",
    "config.world_size = 3\n",
    "config.num_obstacles = 0\n",
    "config.sim_mode = wg.config.SimSpeedMode.FAST\n",
    "config.sim_step_every_x = 10\n",
    "config.relative_action = True\n",
    "config.direction_type = wg.config.DirectionType.STEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../backend/webotsgym/env/webotenv.py:111: UserWarning: Relative property of action class is overwritten by config.relative_action.\n",
      "  warnings.warn(\"Relative property of action class is overwritten \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepting on Port:  10201\n"
     ]
    }
   ],
   "source": [
    "env = wg.WbtGym(train=True, \n",
    "                evaluate_class=MyEval,\n",
    "                action_class = wg.WbtActContinuous(config=config, relative = True),\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:153: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:163: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:191: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_name = \"3x3_reach_target_v7\"\n",
    "model = PPO1(\"MlpPolicy\", env, timesteps_per_actorbatch = 5000, tensorboard_log=\"./{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/anaconda3/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "reward boundary, reward:  -5086.918606296512\n",
      "Calc_reward target bonus:  7574.323943505684\n",
      "target reached, reward:  -1162.1049346226625\n",
      "reward boundary, reward:  -5000.535842304488\n",
      "reward boundary, reward:  -5058.637903784529\n",
      "reward boundary, reward:  -5003.04422231269\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    model.learn(total_timesteps=500000)\n",
    "    model.save(\"./model_{}\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
